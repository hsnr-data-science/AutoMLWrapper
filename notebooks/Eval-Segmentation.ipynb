{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3cdca06-c4e0-4fe1-8ddf-c5d2a80148be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "TENSORFLOW = 1\n",
    "GLUON = 1\n",
    "\n",
    "import os  \n",
    "if GLUON:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "if TENSORFLOW:\n",
    "    import tensorflow as tf\n",
    "\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "            tf.config.set_logical_device_configuration(\n",
    "                gpus[0],\n",
    "                [tf.config.LogicalDeviceConfiguration(memory_limit=24000)])\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7be377c-7b56-4648-97aa-e473fa1c9c51",
   "metadata": {},
   "source": [
    "1 Physical GPUs, 1 Logical GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8ecfa31-8621-4bb9-884d-c31fd72a3baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, pandas as pd, requests, mlflow, sys, os, logging, numpy as np\n",
    "from automlwrapper import AutoMLWrapper, SedarDataLoader\n",
    "from sedarapi import SedarAPI\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autogluon.multimodal.utils.object_detection import from_coco\n",
    "from PIL import Image\n",
    "\n",
    "logging.getLogger('automlwrapper').setLevel(logging.ERROR)\n",
    "\n",
    "np.random.seed(1313)\n",
    "\n",
    "TIME_LIMIT = 60 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85666f08-70b7-4a27-9d5f-3adbd1d3da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MLFLOW = False\n",
    "\n",
    "MLFLOW_URI = 'http://192.168.220.107:6798'\n",
    "MLFLOW_EXPID = '1'\n",
    "if USE_MLFLOW:\n",
    "    mlflow.set_tracking_uri(MLFLOW_URI) \n",
    "    mlflow.set_experiment(experiment_id=MLFLOW_EXPID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68f07af0-4546-428f-974a-3ac47ec39e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEDAR_URI = 'http://192.168.220.107:5000'\n",
    "\n",
    "SEDAR = SedarAPI(SEDAR_URI)\n",
    "DataLoader = SedarDataLoader(SEDAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7a79d97-dbaf-45d0-8497-85ca8c2b2677",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_location = './tmp/data/gitterschnitte_segmentation_train'\n",
    "test_location = './tmp/data/gitterschnitte_segmentation_test'\n",
    "val_location = './tmp/data/gitterschnitte_segmentation_val'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e19b04-79f4-4300-b746-2b09aa6939da",
   "metadata": {},
   "source": [
    "## download data to the above locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbca3513-2224-40ab-b2bb-7d8daba0542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 7173k  100 7173k    0     0  16.8M      0 --:--:-- --:--:-- --:--:-- 16.8M\n"
     ]
    }
   ],
   "source": [
    "!curl --create-dirs -O --output-dir \\\n",
    "./tmp/data/gitterschnitte_segmentation_train \\\n",
    "https://gitlab.com/mibbels/automlwrapperdata/-/raw/main/image-segmentation/gitterschnitte_segmentation_train.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a1f113e-768b-4527-ad57-3073eda18355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  345k  100  345k    0     0   977k      0 --:--:-- --:--:-- --:--:--  979k\n"
     ]
    }
   ],
   "source": [
    "!curl --create-dirs -O --output-dir \\\n",
    "./tmp/data/gitterschnitte_segmentation_test \\\n",
    "https://gitlab.com/mibbels/automlwrapperdata/-/raw/main/image-segmentation/gitterschnitte_segmentation_test.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03a1dc8-5ca9-4dab-bfb6-502a0c87b1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1337k  100 1337k    0     0  3503k      0 --:--:-- --:--:-- --:--:-- 3502k\n"
     ]
    }
   ],
   "source": [
    "!curl --create-dirs -O --output-dir \\\n",
    "./tmp/data/gitterschnitte_segmentation_val \\\n",
    "https://gitlab.com/mibbels/automlwrapperdata/-/raw/main/image-segmentation/gitterschnitte_segmentation_val.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1681464a-4b9b-4706-8503-9059004920a0",
   "metadata": {},
   "source": [
    "## Read the segmentation dataset from files\n",
    "#### Same code utilizing the SEDAR API:\n",
    "```python\n",
    "DataLoader.query_data('<workspace id>',\n",
    "                      '<dataset id>',\n",
    "                      file_save_location = test_location)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf7b0ff2-278f-4a5a-9e15-43eea3aca3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataLoader.segmentation_as_data_frame(train_location, train_location + '/unzip')\n",
    "\n",
    "test = DataLoader.segmentation_as_data_frame(test_location, test_location + '/unzip')\n",
    "\n",
    "val = DataLoader.segmentation_as_data_frame(val_location, val_location + '/unzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9a81ed4-4560-42d6-925a-83fa7edd25c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>./tmp/data/gitterschnitte_segmentation_train/u...</td>\n",
       "      <td>./tmp/data/gitterschnitte_segmentation_train/u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    image  \\\n",
       "count                                                 174   \n",
       "unique                                                174   \n",
       "top     ./tmp/data/gitterschnitte_segmentation_train/u...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                                    label  \n",
       "count                                                 174  \n",
       "unique                                                174  \n",
       "top     ./tmp/data/gitterschnitte_segmentation_train/u...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d6c4740-8c58-4542-bc5c-8b4f790cfab6",
   "metadata": {},
   "source": [
    " \timage \tlabel\n",
    "count \t174 \t174\n",
    "unique \t174 \t174\n",
    "top \t./tmp/data/gitterschnitte_segmentation_train/u... \t./tmp/data/gitterschnitte_segmentation_train/u...\n",
    "freq \t1 \t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "687f3fec-fb17-4144-9bdc-e4f778944f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>./tmp/data/gitterschnitte_segmentation_test/un...</td>\n",
       "      <td>./tmp/data/gitterschnitte_segmentation_test/un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    image  \\\n",
       "count                                                   8   \n",
       "unique                                                  8   \n",
       "top     ./tmp/data/gitterschnitte_segmentation_test/un...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                                    label  \n",
       "count                                                   8  \n",
       "unique                                                  8  \n",
       "top     ./tmp/data/gitterschnitte_segmentation_test/un...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dbcaac6-4982-42b4-ba3d-f1ad74b46743",
   "metadata": {},
   "source": [
    " \timage \tlabel\n",
    "count \t8 \t8\n",
    "unique \t8 \t8\n",
    "top \t./tmp/data/gitterschnitte_segmentation_test/un... \t./tmp/data/gitterschnitte_segmentation_test/un...\n",
    "freq \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e09fe0a-7664-46a4-b8c3-b77139d7efb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>./tmp/data/gitterschnitte_segmentation_val/unz...</td>\n",
       "      <td>./tmp/data/gitterschnitte_segmentation_val/unz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    image  \\\n",
       "count                                                  35   \n",
       "unique                                                 35   \n",
       "top     ./tmp/data/gitterschnitte_segmentation_val/unz...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                                    label  \n",
       "count                                                  35  \n",
       "unique                                                 35  \n",
       "top     ./tmp/data/gitterschnitte_segmentation_val/unz...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49d7827c-fde7-4898-b1ae-1fb93183f43f",
   "metadata": {},
   "source": [
    " \timage \tlabel\n",
    "count \t35 \t35\n",
    "unique \t35 \t35\n",
    "top \t./tmp/data/gitterschnitte_segmentation_val/unz... \t./tmp/data/gitterschnitte_segmentation_val/unz...\n",
    "freq \t1 \t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356508b8-5693-4cb7-a4a5-22201c37a4a9",
   "metadata": {},
   "source": [
    "# Optimizing the smallest version of the SAM-Network using AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03760511-86dd-4df9-837d-249628b21d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper(train_data, val_data, eval_metric):\n",
    "    \n",
    "    wrapper = AutoMLWrapper('autogluon')\n",
    "    wrapper.AllowExtraHyperparameters({'constructor': ['hyperparameters']})\n",
    "    wrapper.Train(\n",
    "        train_data=train_data,\n",
    "        validation_data=val_data,\n",
    "        target_column='label',\n",
    "        task_type='segmentation',\n",
    "        data_type='image',\n",
    "        problem_type='segmentation',\n",
    "        hyperparameters={'eval_metric':eval_metric,\n",
    "                         'validation_metric':eval_metric,\n",
    "                         'time_limit':TIME_LIMIT,\n",
    "                         'verbosity':0, \n",
    "                         'hyperparameters':{\n",
    "                             \"model.sam.checkpoint_name\": \"facebook/sam-vit-base\",\n",
    "                         }\n",
    "                        }\n",
    "    )\n",
    "    \n",
    "    if USE_MLFLOW:\n",
    "         additional_tags = {\n",
    "         \"workspace\":\"DefaultWorkspace,13b4787c3e454649aa05a4cd680edc37\",\n",
    "         \"notebook\": \"glass classifiaction PB,854c94fd-1898-4f03-b0c6-b60b5571b102\",\n",
    "         \"is_public\":\"true\",\n",
    "         \"datasets\":\"glass_schema|986f2e837ca44f3e8c0ee7d2dc0c4287\",\n",
    "         \"username\":\"mibbels\",\n",
    "         }\n",
    "         n = 3\n",
    "         wrapper.MlflowUploadTopN(n, additional_tags)\n",
    "\n",
    "    return wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "692bed0d-fc02-4a0d-8808-b1aec7cd963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoMLWrapper: Warning - num_classes not found in params. Setting num_classes to 1 in order to use binary_iou as eval_metric.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name              | Type                       | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model             | SAMForSemanticSegmentation | 93.4 M\n",
      "1 | validation_metric | Binary_IoU                 | 0     \n",
      "2 | loss_func         | StructureLoss              | 0     \n",
      "-----------------------------------------------------------------\n",
      "3.6 M     Trainable params\n",
      "89.8 M    Non-trainable params\n",
      "93.4 M    Total params\n",
      "373.703   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/automl/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4674612397ec48b78a7b763566ac698b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 44: 'val_iou' reached 0.82926 (best 0.82926), saving model to '/home/jovyan/AutoMLOutput/autogluon1712601669.572252/epoch=0-step=44.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 88: 'val_iou' reached 0.86213 (best 0.86213), saving model to '/home/jovyan/AutoMLOutput/autogluon1712601669.572252/epoch=1-step=88.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 132: 'val_iou' reached 0.81445 (best 0.86213), saving model to '/home/jovyan/AutoMLOutput/autogluon1712601669.572252/epoch=2-step=132.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 176: 'val_iou' reached 0.85576 (best 0.86213), saving model to '/home/jovyan/AutoMLOutput/autogluon1712601669.572252/epoch=3-step=176.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 220: 'val_iou' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 264: 'val_iou' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 308: 'val_iou' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 352: 'val_iou' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 396: 'val_iou' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 440: 'val_iou' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 484: 'val_iou' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 528: 'val_iou' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 39s, sys: 16.4 s, total: 12min 56s\n",
      "Wall time: 8min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w = wrapper(train, val, 'iou')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07d12594-1c20-41c5-b160-f750a1d1bc7c",
   "metadata": {},
   "source": [
    "  | Name              | Type                       | Params\n",
    "-----------------------------------------------------------------\n",
    "0 | model             | SAMForSemanticSegmentation | 93.4 M\n",
    "1 | validation_metric | Binary_IoU                 | 0     \n",
    "2 | loss_func         | StructureLoss              | 0     \n",
    "-----------------------------------------------------------------\n",
    "3.6 M     Trainable params\n",
    "89.8 M    Non-trainable params\n",
    "93.4 M    Total params\n",
    "373.703   Total estimated model params size (MB)\n",
    "\n",
    "/usr/local/automl/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
    "  warnings.warn(warning.format(ret))\n",
    "\n",
    "Epoch 11: 100%\n",
    "174/174 [00:43<00:00, 4.02it/s]\n",
    "\n",
    "Epoch 0, global step 44: 'val_iou' reached 0.82926 (best 0.82926), saving model to '/home/jovyan/AutoMLOutput/autogluon1712601669.572252/epoch=0-step=44.ckpt' as top 3\n",
    "\n",
    "Epoch 1, global step 88: 'val_iou' reached 0.86213 (best 0.86213), saving model to '/home/jovyan/AutoMLOutput/autogluon1712601669.572252/epoch=1-step=88.ckpt' as top 3\n",
    "\n",
    "Epoch 2, global step 132: 'val_iou' reached 0.81445 (best 0.86213), saving model to '/home/jovyan/AutoMLOutput/autogluon1712601669.572252/epoch=2-step=132.ckpt' as top 3\n",
    "\n",
    "Epoch 3, global step 176: 'val_iou' reached 0.85576 (best 0.86213), saving model to '/home/jovyan/AutoMLOutput/autogluon1712601669.572252/epoch=3-step=176.ckpt' as top 3\n",
    "\n",
    "Epoch 4, global step 220: 'val_iou' was not in top 3\n",
    "\n",
    "Epoch 5, global step 264: 'val_iou' was not in top 3\n",
    "\n",
    "Epoch 6, global step 308: 'val_iou' was not in top 3\n",
    "\n",
    "Epoch 7, global step 352: 'val_iou' was not in top 3\n",
    "\n",
    "Epoch 8, global step 396: 'val_iou' was not in top 3\n",
    "\n",
    "Epoch 9, global step 440: 'val_iou' was not in top 3\n",
    "\n",
    "Epoch 10, global step 484: 'val_iou' was not in top 3\n",
    "\n",
    "Epoch 11, global step 528: 'val_iou' was not in top 3\n",
    "\n",
    "CPU times: user 12min 39s, sys: 16.4 s, total: 12min 56s\n",
    "Wall time: 8min 43s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8851f77c-36b7-4624-8f00-378385d979fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe546230d8fd481181e2f3f128d23dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iou': 0.818583607673645}\n",
      "CPU times: user 6.78 s, sys: 1.07 s, total: 7.85 s\n",
      "Wall time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_iou = w.Evaluate(test, target_column='label', metrics=['iou'])\n",
    "\n",
    "print(test_iou)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0365faa-eaa8-4f47-896b-794797546ae0",
   "metadata": {},
   "source": [
    "Predicting DataLoader 0: 100%\n",
    "8/8 [00:00<00:00, 9.95it/s]\n",
    "\n",
    "{'iou': 0.818583607673645}\n",
    "CPU times: user 6.78 s, sys: 1.07 s, total: 7.85 s\n",
    "Wall time: 1.82 s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8a79c0-fce8-4660-9aee-c2039012db33",
   "metadata": {},
   "source": [
    "--------------\n",
    "# Evaluating the LLMs capabilities when creating segmentation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4c631-6be2-410e-b333-a3ab452baa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataLoader.segmentation_as_data_frame(train_location, train_location + '/unzip')\n",
    "\n",
    "test = DataLoader.segmentation_as_data_frame(test_location, test_location + '/unzip')\n",
    "\n",
    "val = DataLoader.segmentation_as_data_frame(val_location, val_location + '/unzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f4a42-bfcd-4417-8244-f8ef9f194b28",
   "metadata": {},
   "source": [
    "### **Trial 1**: The prompt currently used within the SEDAR System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5157a76-3671-40c6-8ec1-a74fac58650c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         \n",
      "Generate a Python code snippet for an Image Segmentation pipeline using the libraries Tensorflow or Pytorch and other appropriate libraries, optimized for GPU usage. \n",
      "The code should handle a dataframe called gitter_trainv1 with columns 'image' and 'label'. 'image' contains the path to the image file and 'label' contains the path to the mask file.\n",
      "The dataset contains 348 images. \n",
      "All images are 256px in height, 256px in width, in GRAY mode, and saved as png.\n",
      "\n",
      "Focus on a simple yet effective neural network architecture suitable for beginners. You may use a pre-trained model and fine-tune it on the dataset.\n",
      "Include necessary imports, dataset loading and preprocessing steps, model architecture, and training loop. The code should be ready to run in a Python interpreter without modifications.\n",
      "Annotate the code with comments to explain each step and give hints for continued development of that pipeline.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''                                         \n",
    "Generate a Python code snippet for an Image Segmentation pipeline using the libraries Tensorflow or Pytorch and other appropriate libraries, optimized for GPU usage. \n",
    "The code should handle a dataframe called gitter_trainv1 with columns 'image' and 'label'. 'image' contains the path to the image file and 'label' contains the path to the mask file.\n",
    "The dataset contains 348 images. \n",
    "All images are 256px in height, 256px in width, in GRAY mode, and saved as png.\n",
    "\n",
    "Focus on a simple yet effective neural network architecture suitable for beginners. You may use a pre-trained model and fine-tune it on the dataset.\n",
    "Include necessary imports, dataset loading and preprocessing steps, model architecture, and training loop. The code should be ready to run in a Python interpreter without modifications.\n",
    "Annotate the code with comments to explain each step and give hints for continued development of that pipeline.\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ce1f36-2548-4e74-96e0-7770f6aaff1d",
   "metadata": {},
   "source": [
    "# When presented with that prompt, OpenOrca tries to create a classification network \n",
    "- the term 'label' as the name for the column with the mask image might confuse the LLM\n",
    "- `tf.keras.preprocessing.image.load_filelist` does not exist\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# Load the dataset into a pandas dataframe\n",
    "(df,) = tf.keras.preprocessing.image.load_filelist(\n",
    "    gitter_trainv1['image'], \n",
    "    target_size=(256, 256), \n",
    "    batch_size=32, \n",
    "    class_mode='categorical', \n",
    "    subset='training'\n",
    ")\n",
    "#Reshape the data to have channels last\n",
    "df = tf.keras.applications.mobilenet_v2.preprocess_input(df)\n",
    "#Labels are one-hot encoded\n",
    "df['label'] = keras.utils.to_categorical(df['label'], 10)\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(256, 256)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)), layers.Flatten(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(.5),\n",
    "    layers.Dense(10) \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "#Train the model for a specified number of epochs\n",
    "model.fit(df['image'], df['label'], batch_size=32, epochs=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3091069-ce14-4088-85ae-0c596f36a3cd",
   "metadata": {},
   "source": [
    "# # **Trial 2**: The next step was to build the pipeline iteratively with the help of OpenOrca \n",
    "### Step 1: data conversion into arrays\n",
    "Prompt 1:\n",
    "\n",
    "I have a dataframe called gitter_trainv1 with columns 'image' and 'label'. 'image' contains the path to the image file and 'label' contains the path to the mask file. The dataset contains 174 images. All images are 256px in height, 256px in width, in GRAY mode, and saved as png. Write the python code that loads the images and masks into numpy arrays, so that I can use them to create a neural network for image segmentation in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05ed76c2-a7d7-4f63-b31a-70b93752bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Loading Images and Masks from Dataframe\n",
    "data_df = train\n",
    "image_paths = list(data_df['image'])   # Extract image paths\n",
    "mask_paths = list(data_df['label'])    # Extract mask paths\n",
    "\n",
    "images = np.array([cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) for img_path in image_paths]) / 255.0\n",
    "images = images.reshape(-1, 256, 256, 1)\n",
    "    \n",
    "labels = np.array([np.array(Image.open(mask_path).convert('L').resize((256, 256))) for mask_path in mask_paths]) / 255.0\n",
    "labels = labels.reshape(-1, 256, 256, 1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b3597d-ec2d-4e27-9630-642955e5a1fc",
   "metadata": {},
   "source": [
    "### Step 2: creation of tensorflow dataset\n",
    "\n",
    "Prompt 2:\n",
    "\n",
    "Create a Tensorflow dataset from those arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1821135-194c-4f8a-8457-39157318f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming 'train' is your DataFrame with 'image' and 'label' columns\n",
    "image_paths = list(train['image'])  # Extract image paths\n",
    "mask_paths = list(train['label'])   # Extract mask paths\n",
    "\n",
    "def create_ds(image_paths, mask_paths):\n",
    "    # Load images and masks, ensuring they are the same size and normalized\n",
    "    images = np.array([cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) for img_path in image_paths]) / 255.0\n",
    "    images = images.reshape(-1, 256, 256, 1)\n",
    "    \n",
    "    labels = np.array([np.array(Image.open(mask_path).convert('L').resize((256, 256))) for mask_path in mask_paths]) / 255.0\n",
    "    labels = labels.reshape(-1, 256, 256, 1) \n",
    "    # Create a TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels)).batch(4)\n",
    "    return dataset\n",
    "\n",
    "train = create_ds(image_paths, mask_paths)\n",
    "\n",
    "test = create_ds(list(test['image']), list(test['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da847ee3-186b-45af-9354-8a60fb5abb46",
   "metadata": {},
   "source": [
    "## Fix after Step 3:\n",
    "#### positioned above in the code\n",
    "\n",
    "Prompt 4:\n",
    "\n",
    "create_block() in create_unet() was not defined\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d796ea9-9bdc-4812-b1e3-f25de6ab6721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_block(x, filters, mode):\n",
    "    if mode == \"down\":\n",
    "        x = layers.Conv2D(filters, 3, activation='relu', padding='same')(x)\n",
    "        x = layers.MaxPooling2D()(x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "    elif mode == \"up\":\n",
    "        x = layers.Conv2DTranspose(filters, 3, strides=2, activation='relu', padding='same')(x)\n",
    "        #x = layers.UpSampling2D()(x)     # <------ wrong!   // edited\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ac60f-845e-47fa-b7e7-160509ad08bb",
   "metadata": {},
   "source": [
    "# Step 3: network creation\n",
    "\n",
    "Prompt 3:\n",
    "\n",
    "Now give me the code to create a neural network for image segmentation with this dataset using tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c0c1941-7d65-4825-a1c1-cc340a9792a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Define the U-Net architecture\n",
    "def create_unet(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoding path (downsampling)\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    # Downsampling (repeat 4 times)\n",
    "    for i in range(3):\n",
    "        x = create_block(x, 32, 'down')\n",
    "        \n",
    "    # Bottleneck\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    for i in range(1): # <-------- iterator not included in answer, assuming 1 //edited\n",
    "        x = layers.Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same')(x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    # Upsampling (repeat 4 times)\n",
    "    for i in range(3):\n",
    "        x = create_block(x, 32, 'up')\n",
    "        \n",
    "    # Output layer\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32254904-7d34-4335-b17f-80db2960ba70",
   "metadata": {},
   "source": [
    "## Step 4: training\n",
    "\n",
    "Prompt 5:\n",
    "\n",
    "Apply the u-net to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "433b1360-dc37-4390-9c72-d7a512970d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 19:03:58.200370: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_2/dropout_16/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 4s 18ms/step - loss: 0.6935 - binary_io_u_2: 0.2774\n",
      "Epoch 2/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.6925 - binary_io_u_2: 0.2759\n",
      "Epoch 3/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.6917 - binary_io_u_2: 0.2759\n",
      "Epoch 4/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.6927 - binary_io_u_2: 0.3276\n",
      "Epoch 5/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.6824 - binary_io_u_2: 0.2758\n",
      "Epoch 6/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.6808 - binary_io_u_2: 0.2759\n",
      "Epoch 7/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.6428 - binary_io_u_2: 0.2759\n",
      "Epoch 8/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.5148 - binary_io_u_2: 0.5922\n",
      "Epoch 9/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.4364 - binary_io_u_2: 0.7008\n",
      "Epoch 10/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.4031 - binary_io_u_2: 0.7200\n",
      "Epoch 11/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3873 - binary_io_u_2: 0.7354\n",
      "Epoch 12/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3773 - binary_io_u_2: 0.7474\n",
      "Epoch 13/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3696 - binary_io_u_2: 0.7572\n",
      "Epoch 14/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3592 - binary_io_u_2: 0.7631\n",
      "Epoch 15/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3554 - binary_io_u_2: 0.7652\n",
      "Epoch 16/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3467 - binary_io_u_2: 0.7718\n",
      "Epoch 17/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3425 - binary_io_u_2: 0.7738\n",
      "Epoch 18/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3374 - binary_io_u_2: 0.7781\n",
      "Epoch 19/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3343 - binary_io_u_2: 0.7804\n",
      "Epoch 20/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3312 - binary_io_u_2: 0.7826\n",
      "Epoch 21/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3280 - binary_io_u_2: 0.7847\n",
      "Epoch 22/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3257 - binary_io_u_2: 0.7860\n",
      "Epoch 23/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3232 - binary_io_u_2: 0.7875\n",
      "Epoch 24/50\n",
      "44/44 [==============================] - 1s 17ms/step - loss: 0.3197 - binary_io_u_2: 0.7891\n",
      "Epoch 25/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3179 - binary_io_u_2: 0.7902\n",
      "Epoch 26/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3153 - binary_io_u_2: 0.7911\n",
      "Epoch 27/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3128 - binary_io_u_2: 0.7932\n",
      "Epoch 28/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3102 - binary_io_u_2: 0.7947\n",
      "Epoch 29/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3082 - binary_io_u_2: 0.7953\n",
      "Epoch 30/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3065 - binary_io_u_2: 0.7969\n",
      "Epoch 31/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3054 - binary_io_u_2: 0.7972\n",
      "Epoch 32/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3034 - binary_io_u_2: 0.7980\n",
      "Epoch 33/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.3002 - binary_io_u_2: 0.7995\n",
      "Epoch 34/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2986 - binary_io_u_2: 0.8002\n",
      "Epoch 35/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2982 - binary_io_u_2: 0.8011\n",
      "Epoch 36/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2953 - binary_io_u_2: 0.8019\n",
      "Epoch 37/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2947 - binary_io_u_2: 0.8029\n",
      "Epoch 38/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2929 - binary_io_u_2: 0.8038\n",
      "Epoch 39/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2923 - binary_io_u_2: 0.8043\n",
      "Epoch 40/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2908 - binary_io_u_2: 0.8048\n",
      "Epoch 41/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2888 - binary_io_u_2: 0.8055\n",
      "Epoch 42/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2877 - binary_io_u_2: 0.8068\n",
      "Epoch 43/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2849 - binary_io_u_2: 0.8080\n",
      "Epoch 44/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2848 - binary_io_u_2: 0.8081\n",
      "Epoch 45/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2825 - binary_io_u_2: 0.8093\n",
      "Epoch 46/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2820 - binary_io_u_2: 0.8091\n",
      "Epoch 47/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2802 - binary_io_u_2: 0.8105\n",
      "Epoch 48/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2787 - binary_io_u_2: 0.8108\n",
      "Epoch 49/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2775 - binary_io_u_2: 0.8115\n",
      "Epoch 50/50\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 0.2763 - binary_io_u_2: 0.8119\n",
      "CPU times: user 1min 15s, sys: 13.2 s, total: 1min 28s\n",
      "Wall time: 44.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a TensorFlow Dataset from the images and labels.\n",
    "#testdataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(4)\n",
    "\n",
    "# Normalize the input images (optional).\n",
    "#dataset = dataset.map(lambda x, y: (x / 255., y / 255.))\n",
    "\n",
    "#### ^^^ commented out manually\n",
    "\n",
    "#Train the U-Net model with your data.\n",
    "unet_model = create_unet((256, 256,1))     ## was (256,256) // edited\n",
    "unet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.BinaryIoU()])\n",
    "history = unet_model.fit(train, epochs=50)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4bcdc98d-7ea3-45ac-9142-c7920b8ed73b",
   "metadata": {},
   "source": [
    "Epoch 1/50\n",
    "\n",
    "2024-04-08 19:03:58.200370: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_2/dropout_16/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
    "\n",
    "44/44 [==============================] - 4s 18ms/step - loss: 0.6935 - binary_io_u_2: 0.2774\n",
    "Epoch 2/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.6925 - binary_io_u_2: 0.2759\n",
    "Epoch 3/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.6917 - binary_io_u_2: 0.2759\n",
    "Epoch 4/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.6927 - binary_io_u_2: 0.3276\n",
    "Epoch 5/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.6824 - binary_io_u_2: 0.2758\n",
    "Epoch 6/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.6808 - binary_io_u_2: 0.2759\n",
    "Epoch 7/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.6428 - binary_io_u_2: 0.2759\n",
    "Epoch 8/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.5148 - binary_io_u_2: 0.5922\n",
    "Epoch 9/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.4364 - binary_io_u_2: 0.7008\n",
    "Epoch 10/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.4031 - binary_io_u_2: 0.7200\n",
    "Epoch 11/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3873 - binary_io_u_2: 0.7354\n",
    "Epoch 12/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3773 - binary_io_u_2: 0.7474\n",
    "Epoch 13/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3696 - binary_io_u_2: 0.7572\n",
    "Epoch 14/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3592 - binary_io_u_2: 0.7631\n",
    "Epoch 15/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3554 - binary_io_u_2: 0.7652\n",
    "Epoch 16/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3467 - binary_io_u_2: 0.7718\n",
    "Epoch 17/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3425 - binary_io_u_2: 0.7738\n",
    "Epoch 18/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3374 - binary_io_u_2: 0.7781\n",
    "Epoch 19/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3343 - binary_io_u_2: 0.7804\n",
    "Epoch 20/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3312 - binary_io_u_2: 0.7826\n",
    "Epoch 21/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3280 - binary_io_u_2: 0.7847\n",
    "Epoch 22/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3257 - binary_io_u_2: 0.7860\n",
    "Epoch 23/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3232 - binary_io_u_2: 0.7875\n",
    "Epoch 24/50\n",
    "44/44 [==============================] - 1s 17ms/step - loss: 0.3197 - binary_io_u_2: 0.7891\n",
    "Epoch 25/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3179 - binary_io_u_2: 0.7902\n",
    "Epoch 26/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3153 - binary_io_u_2: 0.7911\n",
    "Epoch 27/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3128 - binary_io_u_2: 0.7932\n",
    "Epoch 28/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3102 - binary_io_u_2: 0.7947\n",
    "Epoch 29/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3082 - binary_io_u_2: 0.7953\n",
    "Epoch 30/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3065 - binary_io_u_2: 0.7969\n",
    "Epoch 31/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3054 - binary_io_u_2: 0.7972\n",
    "Epoch 32/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3034 - binary_io_u_2: 0.7980\n",
    "Epoch 33/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.3002 - binary_io_u_2: 0.7995\n",
    "Epoch 34/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2986 - binary_io_u_2: 0.8002\n",
    "Epoch 35/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2982 - binary_io_u_2: 0.8011\n",
    "Epoch 36/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2953 - binary_io_u_2: 0.8019\n",
    "Epoch 37/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2947 - binary_io_u_2: 0.8029\n",
    "Epoch 38/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2929 - binary_io_u_2: 0.8038\n",
    "Epoch 39/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2923 - binary_io_u_2: 0.8043\n",
    "Epoch 40/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2908 - binary_io_u_2: 0.8048\n",
    "Epoch 41/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2888 - binary_io_u_2: 0.8055\n",
    "Epoch 42/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2877 - binary_io_u_2: 0.8068\n",
    "Epoch 43/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2849 - binary_io_u_2: 0.8080\n",
    "Epoch 44/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2848 - binary_io_u_2: 0.8081\n",
    "Epoch 45/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2825 - binary_io_u_2: 0.8093\n",
    "Epoch 46/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2820 - binary_io_u_2: 0.8091\n",
    "Epoch 47/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2802 - binary_io_u_2: 0.8105\n",
    "Epoch 48/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2787 - binary_io_u_2: 0.8108\n",
    "Epoch 49/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2775 - binary_io_u_2: 0.8115\n",
    "Epoch 50/50\n",
    "44/44 [==============================] - 1s 18ms/step - loss: 0.2763 - binary_io_u_2: 0.8119\n",
    "CPU times: user 1min 15s, sys: 13.2 s, total: 1min 28s\n",
    "Wall time: 44.3 s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9716ba73-35de-44fa-a64f-a89038beeffc",
   "metadata": {},
   "source": [
    "### added manually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a37d717-2259-4cfa-81f9-0622d6329df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 11ms/step - loss: 0.1174 - binary_io_u_2: 0.9050\n",
      "Test Loss: 0.1174040213227272\n",
      "Test IoU: 0.9050248861312866\n"
     ]
    }
   ],
   "source": [
    "loss, iou = unet_model.evaluate(test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test IoU: {iou}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "452128ed-d421-45d3-bf0c-88f58f7f34e6",
   "metadata": {},
   "source": [
    "2/2 [==============================] - 0s 11ms/step - loss: 0.1174 - binary_io_u_2: 0.9050\n",
    "Test Loss: 0.1174040213227272\n",
    "Test IoU: 0.9050248861312866"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python automl",
   "language": "python",
   "name": "automl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
