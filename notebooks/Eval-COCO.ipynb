{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "533c1d68-10c2-42c6-8594-29d5d993ee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:46:10.738007: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-09 12:46:10.782976: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-09 12:46:11.573138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:46:13.384986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 24000 MB memory:  -> device: 0, name: Quadro RTX 8000, pci bus id: 0000:5e:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "TENSORFLOW = 1\n",
    "GLUON = 1\n",
    "\n",
    "import os  \n",
    "if GLUON:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "if TENSORFLOW:\n",
    "    import tensorflow as tf\n",
    "\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "            tf.config.set_logical_device_configuration(\n",
    "                gpus[0],\n",
    "                [tf.config.LogicalDeviceConfiguration(memory_limit=24000)])\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1063b58-ae43-4b82-8b13-dd1d66da9f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "WARNING AutoSklearn could not be mported. It might not b available in this environment. Err: \n",
      " No module named 'autosklearn'.\n",
      "WARNING AutoSklearn could not be imported. It might not be available in this environment. Err: \n",
      " No module named 'autosklearn'.\n"
     ]
    }
   ],
   "source": [
    "import warnings, pandas as pd, requests, mlflow, sys, os, logging, numpy as np\n",
    "from automlwrapper import AutoMLWrapper, SedarDataLoader\n",
    "from sedarapi import SedarAPI\n",
    "\n",
    "logging.getLogger('automlwrapper').setLevel(logging.ERROR)\n",
    "\n",
    "np.random.seed(1313)\n",
    "\n",
    "TIME_LIMIT = 60 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85666f08-70b7-4a27-9d5f-3adbd1d3da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MLFLOW = False\n",
    "\n",
    "MLFLOW_URI = 'http://192.168.220.107:6798'\n",
    "MLFLOW_EXPID = '1'\n",
    "if USE_MLFLOW:\n",
    "    mlflow.set_tracking_uri(MLFLOW_URI) \n",
    "    mlflow.set_experiment(experiment_id=MLFLOW_EXPID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336e9d03-3566-4333-be2b-48ef75557614",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEDAR_URI = 'http://192.168.220.107:5000'\n",
    "\n",
    "SEDAR = SedarAPI(SEDAR_URI)\n",
    "DataLoader = SedarDataLoader(SEDAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27771773-4e57-47f9-94f5-6e6831ded586",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_location = './tmp/data/coco_train_balanced'\n",
    "test_location = './tmp/data/coco_test_balanced'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a86df4-03aa-4075-aafc-5ae6a5f92e86",
   "metadata": {},
   "source": [
    "## download data to the above locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f80c6a76-0cb5-4b4e-a99b-4e078a61cc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 5636k  100 5636k    0     0  11.0M      0 --:--:-- --:--:-- --:--:-- 11.0M\n"
     ]
    }
   ],
   "source": [
    "!curl --create-dirs -O --output-dir \\\n",
    "./tmp/data/coco_train_balanced \\\n",
    "https://gitlab.com/mibbels/automlwrapperdata/-/raw/main/object-detection/obj_dect_train_balanced.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d6486c-e9b9-4110-b26e-f85edeffc725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1412k  100 1412k    0     0  3746k      0 --:--:-- --:--:-- --:--:-- 3757k\n"
     ]
    }
   ],
   "source": [
    "!curl --create-dirs -O --output-dir \\\n",
    "./tmp/data/coco_test_balanced \\\n",
    "https://gitlab.com/mibbels/automlwrapperdata/-/raw/main/object-detection/obj_dect_test_balanced.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96291418-4412-498d-90cf-905ac824ff86",
   "metadata": {},
   "source": [
    "## Read the object detection dataset from files\n",
    "#### Same code utilizing the SEDAR API:\n",
    "```python\n",
    "DataLoader.query_data('<workspace id>',\n",
    "                      '<dataset id>',\n",
    "                      file_save_location = test_location)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f06811a0-6b13-40e7-91be-21564453be1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "test = DataLoader.cocoAsDataFrame(zip_path=test_location, unzip_path=test_location+'/unzip')\n",
    "train = DataLoader.cocoAsDataFrame(train_location, train_location + '/unzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df49ea25-142e-4818-9f28-3d00e6d485d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>rois</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>/home/jovyan/tmp/data/coco_train_balanced/unzi...</td>\n",
       "      <td>[[462.999552, 237.999744, 703.0, 461.999935999...</td>\n",
       "      <td>[[462.999552, 237.999744, 703.0, 461.999935999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    image  \\\n",
       "count                                                 140   \n",
       "unique                                                140   \n",
       "top     /home/jovyan/tmp/data/coco_train_balanced/unzi...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                                     rois  \\\n",
       "count                                                 140   \n",
       "unique                                                140   \n",
       "top     [[462.999552, 237.999744, 703.0, 461.999935999...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                                    label  \n",
       "count                                                 140  \n",
       "unique                                                140  \n",
       "top     [[462.999552, 237.999744, 703.0, 461.999935999...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0e01cc1-1b04-4cfd-9bac-332c34ab4460",
   "metadata": {},
   "source": [
    " \timage \trois \tlabel\n",
    "count \t140 \t140 \t140\n",
    "unique \t140 \t140 \t140\n",
    "top \t/home/jovyan/tmp/data/coco_train_balanced/unzi... \t[[462.999552, 237.999744, 703.0, 461.999935999... \t[[462.999552, 237.999744, 703.0, 461.999935999...\n",
    "freq \t1 \t1 \t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c569d16d-9afe-4f7f-8ef6-1360380deecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>rois</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>/home/jovyan/tmp/data/coco_test_balanced/unzip...</td>\n",
       "      <td>[[379.00032, 336.99993600000005, 586.00032, 52...</td>\n",
       "      <td>[[379.00032, 336.99993600000005, 586.00032, 52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    image  \\\n",
       "count                                                  36   \n",
       "unique                                                 36   \n",
       "top     /home/jovyan/tmp/data/coco_test_balanced/unzip...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                                     rois  \\\n",
       "count                                                  36   \n",
       "unique                                                 36   \n",
       "top     [[379.00032, 336.99993600000005, 586.00032, 52...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                                    label  \n",
       "count                                                  36  \n",
       "unique                                                 36  \n",
       "top     [[379.00032, 336.99993600000005, 586.00032, 52...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38d2c8cc-ddae-4246-bb11-7e484bdb4909",
   "metadata": {},
   "source": [
    " \timage \trois \tlabel\n",
    "count \t36 \t36 \t36\n",
    "unique \t36 \t36 \t36\n",
    "top \t/home/jovyan/tmp/data/coco_test_balanced/unzip... \t[[379.00032, 336.99993600000005, 586.00032, 52... \t[[379.00032, 336.99993600000005, 586.00032, 52...\n",
    "freq \t1 \t1 \t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989bb21-e4b2-4ddd-a598-ec664b739ad3",
   "metadata": {},
   "source": [
    "# Optimizing a single predictor using AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab88acd2-c6d0-4eda-a252-bdccb80b4857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_medium(train_data, eval_metric):\n",
    "    wrapper = AutoMLWrapper('autogluon')\n",
    "    \n",
    "    wrapper.Train(\n",
    "        train_data=train_data,\n",
    "        target_column='',\n",
    "        task_type='object-detection',\n",
    "        data_type='image',\n",
    "        problem_type='object-detection',\n",
    "        hyperparameters={'time_limit': TIME_LIMIT, \n",
    "                         'preset' : 'best_quality',\n",
    "                        'eval_metric' : eval_metric,\n",
    "                         'validation_metric': eval_metric,\n",
    "                        'verbosity':1, \n",
    "                        }\n",
    "    )\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527a58d8-d7b0-4541-b09f-3c89f896ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w = wrapper_medium(train,'map')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ee8052b-6331-495d-87e9-8842ec08879b",
   "metadata": {},
   "source": [
    "Global seed set to 0\n",
    "\n",
    "processing dino-5scale_swin-l_8xb2-36e_coco...\n",
    "dino-5scale_swin-l_8xb2-36e_coco-5486e051.pth exists in /home/jovyan\n",
    "Successfully dumped dino-5scale_swin-l_8xb2-36e_coco.py to /home/jovyan\n",
    "processing dino-5scale_swin-l_8xb2-36e_coco...\n",
    "dino-5scale_swin-l_8xb2-36e_coco-5486e051.pth exists in /home/jovyan\n",
    "Successfully dumped dino-5scale_swin-l_8xb2-36e_coco.py to /home/jovyan\n",
    "Loads checkpoint by local backend from path: dino-5scale_swin-l_8xb2-36e_coco-5486e051.pth\n",
    "The model and loaded state dict do not match exactly\n",
    "\n",
    "size mismatch for bbox_head.cls_branches.0.weight: copying a param with shape torch.Size([80, 256]) from checkpoint, the shape in current model is torch.Size([4, 256]).\n",
    "size mismatch for bbox_head.cls_branches.0.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([4]).\n",
    "size mismatch for bbox_head.cls_branches.1.weight: copying a param with shape torch.Size([80, 256]) from checkpoint, the shape in current model is torch.Size([4, 256]).\n",
    "size mismatch for bbox_head.cls_branches.1.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([4]).\n",
    "size mismatch for bbox_head.cls_branches.2.weight: copying a param with shape torch.Size([80, 256]) from checkpoint, the shape in current model is torch.Size([4, 256]).\n",
    "size mismatch for bbox_head.cls_branches.2.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([4]).\n",
    "size mismatch for bbox_head.cls_branches.3.weight: copying a param with shape torch.Size([80, 256]) from checkpoint, the shape in current model is torch.Size([4, 256]).\n",
    "size mismatch for bbox_head.cls_branches.3.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([4]).\n",
    "size mismatch for bbox_head.cls_branches.4.weight: copying a param with shape torch.Size([80, 256]) from checkpoint, the shape in current model is torch.Size([4, 256]).\n",
    "size mismatch for bbox_head.cls_branches.4.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([4]).\n",
    "size mismatch for bbox_head.cls_branches.5.weight: copying a param with shape torch.Size([80, 256]) from checkpoint, the shape in current model is torch.Size([4, 256]).\n",
    "size mismatch for bbox_head.cls_branches.5.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([4]).\n",
    "size mismatch for bbox_head.cls_branches.6.weight: copying a param with shape torch.Size([80, 256]) from checkpoint, the shape in current model is torch.Size([4, 256]).\n",
    "size mismatch for bbox_head.cls_branches.6.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([4]).\n",
    "size mismatch for dn_query_generator.label_embedding.weight: copying a param with shape torch.Size([80, 256]) from checkpoint, the shape in current model is torch.Size([4, 256]).\n",
    "\n",
    "GPU available: True (cuda), used: True\n",
    "TPU available: False, using: 0 TPU cores\n",
    "IPU available: False, using: 0 IPUs\n",
    "HPU available: False, using: 0 HPUs\n",
    "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
    "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
    "\n",
    "  | Name              | Type                             | Params\n",
    "-----------------------------------------------------------------------\n",
    "0 | model             | MMDetAutoModelForObjectDetection | 218 M \n",
    "1 | validation_metric | MeanAveragePrecision             | 0     \n",
    "-----------------------------------------------------------------------\n",
    "23.0 M    Trainable params\n",
    "195 M     Non-trainable params\n",
    "218 M     Total params\n",
    "872.705   Total estimated model params size (MB)\n",
    "\n",
    "/usr/local/automl/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
    "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
    "/usr/local/automl/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Encountered more than 100 detections in a single image. This means that certain detections with the lowest scores will be ignored, that may have an undesirable impact on performance. Please consider adjusting the `max_detection_threshold` to suit your use case. To disable this warning, set attribute class `warn_on_many_detections=False`, after initializing the metric.\n",
    "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
    "\n",
    "Epoch 42: 11%\n",
    "12/112 [00:17<02:24, 1.44s/it]\n",
    "\n",
    "Epoch 0, global step 1: 'val_map' reached 0.00392 (best 0.00392), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=0-step=1.ckpt' as top 1\n",
    "\n",
    "Epoch 1, global step 2: 'val_map' reached 0.01582 (best 0.01582), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=1-step=2.ckpt' as top 1\n",
    "\n",
    "Epoch 2, global step 3: 'val_map' reached 0.02383 (best 0.02383), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=2-step=3.ckpt' as top 1\n",
    "\n",
    "Epoch 3, global step 4: 'val_map' reached 0.03759 (best 0.03759), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=3-step=4.ckpt' as top 1\n",
    "\n",
    "Epoch 4, global step 5: 'val_map' reached 0.05309 (best 0.05309), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=4-step=5.ckpt' as top 1\n",
    "\n",
    "Epoch 5, global step 6: 'val_map' reached 0.06340 (best 0.06340), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=5-step=6.ckpt' as top 1\n",
    "\n",
    "Epoch 6, global step 7: 'val_map' reached 0.09185 (best 0.09185), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=6-step=7.ckpt' as top 1\n",
    "\n",
    "Epoch 7, global step 8: 'val_map' reached 0.12105 (best 0.12105), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=7-step=8.ckpt' as top 1\n",
    "\n",
    "Epoch 8, global step 9: 'val_map' reached 0.14610 (best 0.14610), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=8-step=9.ckpt' as top 1\n",
    "\n",
    "Epoch 9, global step 10: 'val_map' reached 0.17041 (best 0.17041), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=9-step=10.ckpt' as top 1\n",
    "\n",
    "Epoch 10, global step 11: 'val_map' reached 0.20764 (best 0.20764), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=10-step=11.ckpt' as top 1\n",
    "\n",
    "Epoch 11, global step 12: 'val_map' reached 0.25092 (best 0.25092), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=11-step=12.ckpt' as top 1\n",
    "\n",
    "Epoch 12, global step 13: 'val_map' was not in top 1\n",
    "\n",
    "Epoch 13, global step 14: 'val_map' reached 0.25155 (best 0.25155), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=13-step=14.ckpt' as top 1\n",
    "\n",
    "Epoch 14, global step 15: 'val_map' reached 0.25647 (best 0.25647), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=14-step=15.ckpt' as top 1\n",
    "\n",
    "Epoch 15, global step 16: 'val_map' reached 0.27922 (best 0.27922), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=15-step=16.ckpt' as top 1\n",
    "\n",
    "Epoch 16, global step 17: 'val_map' reached 0.29731 (best 0.29731), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=16-step=17.ckpt' as top 1\n",
    "\n",
    "Epoch 17, global step 18: 'val_map' reached 0.30273 (best 0.30273), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=17-step=18.ckpt' as top 1\n",
    "\n",
    "Epoch 18, global step 19: 'val_map' reached 0.32699 (best 0.32699), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=18-step=19.ckpt' as top 1\n",
    "\n",
    "Epoch 19, global step 20: 'val_map' reached 0.36564 (best 0.36564), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=19-step=20.ckpt' as top 1\n",
    "\n",
    "Epoch 20, global step 21: 'val_map' reached 0.37160 (best 0.37160), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=20-step=21.ckpt' as top 1\n",
    "\n",
    "Epoch 21, global step 22: 'val_map' reached 0.48742 (best 0.48742), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=21-step=22.ckpt' as top 1\n",
    "\n",
    "Error displaying widget: model not found\n",
    "\n",
    "Epoch 22, global step 23: 'val_map' reached 0.49018 (best 0.49018), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=22-step=23.ckpt' as top 1\n",
    "\n",
    "Error displaying widget: model not found\n",
    "\n",
    "Epoch 23, global step 24: 'val_map' reached 0.49422 (best 0.49422), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=23-step=24.ckpt' as top 1\n",
    "\n",
    "Error displaying widget: model not found\n",
    "\n",
    "Epoch 24, global step 25: 'val_map' reached 0.49943 (best 0.49943), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=24-step=25.ckpt' as top 1\n",
    "\n",
    "Error displaying widget: model not found\n",
    "\n",
    "Epoch 25, global step 26: 'val_map' reached 0.50505 (best 0.50505), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=25-step=26.ckpt' as top 1\n",
    "\n",
    "Error displaying widget: model not found\n",
    "\n",
    "Epoch 26, global step 27: 'val_map' reached 0.50666 (best 0.50666), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=26-step=27.ckpt' as top 1\n",
    "\n",
    "Error displaying widget: model not found\n",
    "\n",
    "Epoch 27, global step 28: 'val_map' reached 0.50796 (best 0.50796), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=27-step=28.ckpt' as top 1\n",
    "\n",
    "Error displaying widget: model not found\n",
    "\n",
    "Epoch 28, global step 29: 'val_map' reached 0.50947 (best 0.50947), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=28-step=29.ckpt' as top 1\n",
    "\n",
    "Error displaying widget: model not found\n",
    "\n",
    "Epoch 29, global step 30: 'val_map' reached 0.51560 (best 0.51560), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=29-step=30.ckpt' as top 1\n",
    "\n",
    "Error displaying widget: model not found\n",
    "\n",
    "Epoch 30, global step 31: 'val_map' reached 0.51931 (best 0.51931), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=30-step=31.ckpt' as top 1\n",
    "\n",
    "Error displaying widget: model not found\n",
    "\n",
    "Epoch 31, global step 32: 'val_map' reached 0.52228 (best 0.52228), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=31-step=32.ckpt' as top 1\n",
    "\n",
    "Error displaying widget: model not found\n",
    "\n",
    "Epoch 32, global step 33: 'val_map' reached 0.52400 (best 0.52400), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=32-step=33.ckpt' as top 1\n",
    "\n",
    "Error displaying widget: model not found\n",
    "\n",
    "Epoch 33, global step 34: 'val_map' was not in top 1\n",
    "\n",
    "Error displaying widget: model not found\n",
    "\n",
    "Epoch 34, global step 35: 'val_map' reached 0.52473 (best 0.52473), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=34-step=35.ckpt' as top 1\n",
    "\n",
    "Error displaying widget: model not found\n",
    "\n",
    "Epoch 35, global step 36: 'val_map' reached 0.52566 (best 0.52566), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=35-step=36.ckpt' as top 1\n",
    "\n",
    "Epoch 36, global step 37: 'val_map' was not in top 1\n",
    "\n",
    "Epoch 37, global step 38: 'val_map' was not in top 1\n",
    "\n",
    "Epoch 38, global step 39: 'val_map' reached 0.52569 (best 0.52569), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=38-step=39.ckpt' as top 1\n",
    "\n",
    "Epoch 39, global step 40: 'val_map' was not in top 1\n",
    "\n",
    "Epoch 40, global step 41: 'val_map' was not in top 1\n",
    "\n",
    "Epoch 41, global step 42: 'val_map' reached 0.52628 (best 0.52628), saving model to '/home/jovyan/AutoMLOutput/autogluon1712656097.792228/epoch=41-step=42.ckpt' as top 1\n",
    "Time limit reached. Elapsed time is 1:00:00. Signaling Trainer to stop.\n",
    "\n",
    "CPU times: user 2h 14min 15s, sys: 2min 30s, total: 2h 16min 46s\n",
    "Wall time: 1h 27s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979f197-5024-4168-a2a9-f2495ab773af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_map = w.Evaluate(test_data=test, target_column='label')\n",
    "print(test_map)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b0a7431-2095-4a1e-8f25-1cdff0f31efa",
   "metadata": {},
   "source": [
    "{'map': tensor(0.4615), 'map_50': tensor(0.6573), 'map_75': tensor(0.4809), 'map_small': tensor(0.0333), 'map_medium': tensor(0.4712), 'map_large': tensor(0.4474), 'mar_1': tensor(0.2569), 'mar_10': tensor(0.5246), 'mar_100': tensor(0.6250), 'mar_small': tensor(0.1358), 'mar_medium': tensor(0.5461), 'mar_large': tensor(0.5921), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2, 3], dtype=torch.int32), 'mean_average_precision': tensor(0.4615)}\n",
    "CPU times: user 47 s, sys: 851 ms, total: 47.8 s\n",
    "Wall time: 16.2 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eac008-6996-4479-8e05-c5172bd6f8e2",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8a79c0-fce8-4660-9aee-c2039012db33",
   "metadata": {},
   "source": [
    "# Evaluating the LLMs capabilities when creating object detection code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf4287c-6ee9-45e8-8dbb-1a1128f66e7e",
   "metadata": {},
   "source": [
    "## **Trial 1**: An earlier iteration of the prompt, relying on the runtime to add information on the data (coco annotation sample) and added image information from SEDAR \n",
    "-  shorter prompts turned out to be preferred\n",
    "-  'copy and paste' of a serialization of the Neo4j metadata in SEDAR (seen after \"information sample image:\") provides the metadata in a convoluted structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48568df9-10d3-4d49-85cd-ca38a40c318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_MESSAGE = 'locate the correct type of coating-defect in the image'\n",
    "\n",
    "\n",
    "print(f'''Your task is to create the Python code for a Machine Learning Pipeline. You will do that by outputting comment-annotated Python code\n",
    "using the libraries [scikit-learn, Tensorflow, Pytorch].\n",
    "GPUs are available in this environment.\n",
    "\n",
    "Here are some more details on the type of problem the ML Pipeline should solve:                                \n",
    "data_type=image\n",
    "task_type=object-detection\n",
    "problem_type=object-detection\n",
    "The target-variable is: label\n",
    "\n",
    "The Dataset in question is called coco_22_train_v1. \n",
    "\n",
    "\n",
    "\n",
    "        Here is some metadata for the dataset.\n",
    "\n",
    "\n",
    "All data is contained in the dataframe coco_22_train_v1.\n",
    "'rois' and 'label' are the same column. They contain a list of lists with:\n",
    "[[xmin, ymin, xmax, ymax, class], [...]]\n",
    "\n",
    "coco_22_train_v1.iloc[0]:\n",
    "\n",
    "{train.iloc[0]}\n",
    "\n",
    "\n",
    "        information sample image:  <\"id\": \"9b673c7013234b6ea394909bc5a34230\", \"pathFull\": \"coco_22_balanced/images/stippen_blau_2_10_Height.jpg\", \"imageHeight\": \"768\", \"imageWidth\": \"1024\", \"bitsPerPixel\": \"8\", \"colorMode\": \"GREY\", \"format\": \"jpg\">\n",
    "\n",
    "        folder-structure of the dataset zip-file:  <\"uid\": \"e604e262a23d452b8e9145f06595e981\", \"name\": \"coco_22_balanced\", \"type\": \"ROOT\", \"subfolders\": [ <\"uid\": \"378e974a2348413aa2ee4b2af10474f9\", \"name\": \"annotations\", \"type\": \"COCO_ANNOTATION\", \"subfolders\": []>,  <\"uid\": \"5fc438ee133c422d8c325174050f5e89\", \"name\": \"images\", \"type\": \"COCO_IMAGE\", \"subfolders\": []>]>\n",
    "\n",
    "The user was prompted to provide further descriptions. If the text was provided, it will follow now:\n",
    " {YOUR_MESSAGE}                         \n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c01e1-b6b5-4472-8989-25677e2b54ea",
   "metadata": {},
   "source": [
    "#   The above prompt was given to the OS model 'Hermes'\n",
    "Answer 1:\n",
    "\n",
    "I understand that you want me to help you create a Machine Learning Pipeline for object detection using Tensorflow or Pytorch on GPUs with coco_22_train_v1 dataset. The target variable is label and the task type is object detection. Is this correct?\n",
    "\n",
    "Prompt 2:\n",
    "\n",
    "Yes, please give me the code to execute this task.\n",
    "\n",
    "Answer 2:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# Load the dataset into a pandas dataframe.\n",
    "coco_df = pd.read_csv('path/to/dataset.csv')\n",
    "Extract the rois and labels from the dataframe.\n",
    "rois = coco_df['rois'].tolist()\n",
    "labels = coco_df['label'].tolist()\n",
    "Convert the rois to a numpy array of shape (num_rois, 5).\n",
    "num_rois = len(rois)\n",
    "rois_np = np.array(rois)\n",
    "Reshape the rois and labels into tensors for input to the model.\n",
    "rois_tensor = tf.reshape(rois_np, (num_rois, 5))\n",
    "labels_tensor = tf.constant(labels)\n",
    "Define a custom object detection model using Tensorflow.\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32\n",
    "```\n",
    "Prompt 3:\n",
    "\n",
    "Can you finish this code to complete the construction of the neural network?\n",
    "\n",
    "Answer 3:\n",
    "\n",
    " Sure, here's an example of how you can define the rest of the layers in the model and train it on the dataset using Tensorflow on GPUs:\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(.5),\n",
    "    keras.layers.Dense(num_classes, activation='softmax')],\n",
    "name='detection_model',\n",
    "weights=[tf.keras.preprocessing.image.load_img_class_names('path/to/coco_labels.txt'),\n",
    "         tf.keras.applications.mobilenet_v2.decode_predictions])(inputs)\n",
    "# Compile the model with appropriate loss function, optimizer and metrics.\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.Categor\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580567b-30ec-4bf8-8121-50003caa0c54",
   "metadata": {},
   "source": [
    " # **Trial 2**: Below is a custom prompt which tries to convey the task more clearly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a6451-2b3e-4c31-abee-28967d9a9cab",
   "metadata": {},
   "source": [
    "Your task is to create the Python code for a Machine Learning Pipeline. You will do that by outputting comment-annotated Python code using the libraries [scikit-learn, Tensorflow, Pytorch]. GPUs are available in this environment.\n",
    "\n",
    "The code should solve an objkect detection problem with a dataset in COCO format.\n",
    "\n",
    "The COCO file is located at: tmp/data/coco_script_trainv1/unzip/coco_script_trainv1/annotations/labels.json There are 314 images.\n",
    "\n",
    "This are the category names: \"categories\": [ { \"id\": 0, \"name\": \"kocher\" }, { \"id\": 1, \"name\": \"krater\" }, { \"id\": 2, \"name\": \"stippen\" }, { \"id\": 3, \"name\": \"blasen\" } ]\n",
    "\n",
    "Here is information about the format of each image: imageHeight: 768px, imageWidth: 1024px, colorMode: greyscale format: jpg\n",
    "\n",
    "Create the python code for object detection with a deep neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016a4078-70c8-470e-814f-4cab482b7ceb",
   "metadata": {},
   "source": [
    "# Hermes's code was again cut short and we needed to re-prompt\n",
    "\n",
    " Sure! Here's an example code that uses Tensorflow to train a Faster R-CNN model on COCO dataset:\n",
    "```python\n",
    "# Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Loading the COCO dataset\n",
    "import cocoapi.CocoDetection\n",
    "detections = cocoapi.COCO(tmp/data/coco_script_trainv1/unzip/coco_script_trainv1/annotations/instances_val2017.json)\n",
    "# Splitting the dataset into training and validation sets\n",
    "train_idx, val_idx = train_test_split(detections.ids, test_size=0.2, random_state=42)\n",
    "Training the Faster R-RCNN model using Tensorflow\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(76\n",
    "```\n",
    "\n",
    "Prompt 2:\n",
    "\n",
    "Continue giving me the Code for building and training the Faster R-CNN model using Tensorflow\n",
    "\n",
    "Answer 2:\n",
    "Sure! Here's the rest of the code:\n",
    "```python\n",
    ", 1024)),\n",
    "layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "layers.Flatten(), \n",
    "layers.Dense(1024, activation='relu'), \n",
    "layers.Dropout(.5), \n",
    "layers.Dense(314) ])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[tf.keras.metrics.DiceLoss()], ) \n",
    "model.fit(train_dataset, epochs=5)\n",
    "Evaluating the model on validation set\n",
    "\n",
    "val_score = model.evaluate(val_dataset, verbose=0) print('\\nFinal evaluation results:') print('Test loss:', val_score[0\n",
    "```\n",
    "\n",
    "\n",
    "#### The provided code does not try to load the data from the DataFrame, instead it relies on a label file using a made-up library\n",
    "#### The layers as well as metric and loss are not fit for object detection  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c9892-a538-4175-bf7e-22d05c7ec1e2",
   "metadata": {},
   "source": [
    "# **Trial 3**: This next prompt relies on the COCO annotation file \n",
    "\n",
    "Generate a Python code snippet for an object detection pipeline using either TensorFlow or PyTorch, optimized for GPU usage. The code should handle a COCO-format dataset located at \"tmp/data/coco_script_trainv1/unzip/coco_script_trainv1/annotations/labels.json\". The dataset contains 314 images. All images are 768px in height, 1024px in width, in greyscale, and saved as JPGs. Focus on a simple yet effective neural network architecture suitable for beginners. Include necessary imports, dataset loading and preprocessing steps, model architecture, and training loop. The code should be ready to run in a Python interpreter without modifications Annotate the code with comments to explain each step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb39c01-2987-4956-a168-d92d4461aa06",
   "metadata": {},
   "source": [
    "## Here we used OpenOrca as the OS LLM\n",
    "Here's an example of a simple yet effective neural network architecture for object detection using TensorFlow on GPU:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "# Download the COCO dataset:\n",
    "coco_dataset = \"tmp/data/coco_script_trainv1/unzip/coco_script_trainv1/annotations/labels.json\"\n",
    "\n",
    "Preprocess the images and labels for training:\n",
    "def preprocess(img, label):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    return tf.expand_dims(img_array, 0), label\n",
    "\n",
    "Load the dataset using ImageDataGenerator:\n",
    "train_datagen = ImageDataGenerator()\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=\"tmp/data/coco_script_trainv1/unzip/coco_script\n",
    "```\n",
    "\n",
    "Prompt 2:\n",
    "\n",
    "Continue to write this code for loading a coco dataset using ImageDataGenerator.\n",
    "\n",
    "Answer 2:\n",
    "```python\n",
    " trainv1/annotations\", x_col='filename', y_col='labels') class_mode=\"sparse\")\n",
    "```\n",
    "\n",
    "Prompt 3:\n",
    "\n",
    "Build the neural network architecture\n",
    "\n",
    "Answer 3:\n",
    "```python\n",
    "model = keras.Sequential([ layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "                          layers.MaxPooling2D((2, 2)),\n",
    "                          layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "                          layers.MaxPooling2D((2, 2)),\n",
    "                          layers.Flatten(),\n",
    "                          layers.Dense(1024, activation='relu'),\n",
    "                          layers.Dense(len(class_names), activation='softmax')\n",
    "                         ])\n",
    "\n",
    "#Compile the model and train it:\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_generator, epochs=10)\n",
    "\n",
    "\n",
    "#This code snippet demonstrates a simple yet effective neural network architecture for object detection using TensorFlow on GPU. It loads the COCO dataset from the specified path\n",
    "```\n",
    "\n",
    "#### It becomes evident that the task should be split across multiple prompts, each addressing specific problems like data-loading, model creation, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c857db-a1f4-46ac-a4b3-c77dec9b9df7",
   "metadata": {},
   "source": [
    "# **Trial 4**:This custom prompt tackels only data loading from the coco annotatios file\n",
    "\n",
    "\"tmp/data/coco_script_trainv1/unzip/coco_script_trainv1/annotations/labels.json\" contains the annotation file for an object detection dataset in coco format. Provide the code to load this dataset and turn it into a format compatible with tensorflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62998468-8c31-43fb-b3db-c7419c0a0436",
   "metadata": {},
   "source": [
    "## Orca seems to fail without a more detailed description on the structure of the file\n",
    "\n",
    "```python\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def convert_to_tfrecord(data):\n",
    "    for image_id, annotations in data.items():        \n",
    "        print(image_id)\n",
    "        print(annotations)\n",
    "        width, height = Image.open(image_id).size\n",
    "        xmin, ymin, xmax, ymax = [int(float(x)) for x in annotations['bbox']]\n",
    "        class_name = annotations['category']['name']\n",
    "        label = int(annotations['category']['id'])\n",
    "        tfrecord = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'image/height': dataset.tensor_info(width),\n",
    "            'image/width': dataset.tensor_info(height),\n",
    "            'image/filename': dataset.string_list_feature([image_id]),\n",
    "        'image/object/bbox/xmin': dataset.float_list_feature([xmin]),\n",
    "        'image/object/bbox/ymin': dataset.float_list_feature([ymin]),\n",
    "        'image/object/bbox/xmax': dataset.float_list_feature([xmax]),\n",
    "        'image/object/bbox/ymax': dataset.float_list_feature([ymax]),\n",
    "        'image/object/class/label': dataset.int64_list_feature([label]),\n",
    "        'image/object/class/text': dataset.string_list_feature([class_name])\n",
    "    }))\n",
    "    serialized = tfrecord.SerializeToString()\n",
    "    with tf.io.TFRecordWriter(filepath) as writer:\n",
    "        writer.write(serialized)\n",
    "\n",
    "coco_dataset = \"tmp/data/coco_script_trainv1/unzip/coco_script_trainv1/annotations/labels.json\"\n",
    "\n",
    "data = load_json(coco_dataset)\n",
    "\n",
    "convert_to_tfrecord(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb5a319-36dd-4312-a540-d5635583cace",
   "metadata": {},
   "source": [
    "# **Trial 5**: Final fully functional pipeline with GPT4, achieved by prompting each step seperatly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d2b9f66-1873-4006-a390-a68b21055b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision import transforms\n",
    "\n",
    "class CocoDetection(Dataset):\n",
    "    def __init__(self, root, annotation, transform=None):\n",
    "        self.root = root\n",
    "        self.coco = COCO(annotation)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('L')  # Convert to grayscale\n",
    "        \n",
    "        boxes = [ann['bbox'] for ann in coco_annotation]\n",
    "        categories = [ann['category_id'] for ann in coco_annotation]          # EDITED   Added (labels was wrongly torch.ones)\n",
    "        categories = torch.as_tensor(categories, dtype=torch.int64)\n",
    "        \n",
    "        # Convert boxes (x, y, width, height) to (x_min, y_min, x_max, y_max)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "\n",
    "        # Assuming all instances are not crowd\n",
    "        #labels = torch.ones((len(boxes),), dtype=torch.int64)     #EDITED  wrong!\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        #target['labels'] = labels\n",
    "        target['labels'] = categories                        #EDITED \n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "                \n",
    "        return img, target, img_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bce761fe-e62d-4c78-a27c-71dccdaa2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_data(train_path, test_path):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from torch.utils.data._utils.collate import default_collate\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Converts PIL.Image or np.ndarray to FloatTensor of shape (C x H x W) and normalize to [0.0, 1.0]\n",
    "        # Add any other transformations here (e.g., resizing, normalization)\n",
    "    ])\n",
    "    \n",
    "    def collate_fn(batch):   # Result of question for RuntimeError in Evaluate Loop \n",
    "        \"\"\"\n",
    "        Custom collate function for handling None or varying sizes of bounding box annotations.\n",
    "        \"\"\"\n",
    "        batch = list(filter(lambda x: x is not None, batch))  # Remove None samples if any\n",
    "        images = [item[0] for item in batch]\n",
    "        targets = [item[1] for item in batch]\n",
    "        \n",
    "        images = default_collate(images)  # Use PyTorch's default_collate to handle images\n",
    "        \n",
    "        # For targets, simply return the list without attempting to stack\n",
    "        \n",
    "        # You might need to handle targets differently based on your specific needs\n",
    "        return images, targets\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_full = CocoDetection(root=train_path, annotation=train_path+'annotations/labels.json', transform=transform)\n",
    "    \n",
    "    indices = list(range(len(train_full)))\n",
    "    \n",
    "    # Split indices into training and validation sets\n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create subset instances for training and validation\n",
    "    train_dataset = torch.utils.data.Subset(train_full, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(train_full, val_indices)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,  collate_fn=lambda x: tuple(zip(*x)))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "    \n",
    "    test_dataset = CocoDetection(root=test_path, annotation=test_path + 'annotations/labels.json', transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))#collate_fn=collate_fn)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2a2ccaf-c263-4fff-af95-5b07c2f048f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(device, data_loader, num_epochs = 50, pretrained=True):\n",
    "    from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "    import torchvision\n",
    "    \n",
    "    def get_model_instance_segmentation(num_classes):\n",
    "        model = fasterrcnn_resnet50_fpn(pretrained)       #EDITED added variable\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "        return model\n",
    "    \n",
    "    model = get_model_instance_segmentation(num_classes=5) # for example   ### HAS TO BE EDITED!!  \n",
    "    \n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    \n",
    "    # Training loop\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        i = 0\n",
    "        for images, targets, image_ids  in data_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    \n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"Iteration #{i} loss: {losses.item()}\")\n",
    "            i += 1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2b94a6b-d6a8-480d-aca7-8760fe397748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(device, model, data_loader, dataset):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    import json\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets, image_ids in data_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "            outputs = model(images)\n",
    "    \n",
    "            for i, output in enumerate(outputs):\n",
    "                image_id = image_ids[i]#.item()  # Assuming image_ids is a tensor, adjust if it's in another format\n",
    "                for box, label, score in zip(output['boxes'], output['labels'], output['scores']):\n",
    "                    # Convert box from [x1, y1, x2, y2] to [x, y, width, height]\n",
    "                    box = box.cpu().numpy()\n",
    "                    x_min, y_min, x_max, y_max = box\n",
    "                    width, height = x_max - x_min, y_max - y_min\n",
    "                    \n",
    "                    result = {                      # EDITED:: float() added, error encoding np.float32 \n",
    "                        \"image_id\": image_id,\n",
    "                        \"category_id\": label.item(),\n",
    "                        \"bbox\": [float(x_min), float(y_min), float(width), float(height)],\n",
    "                        \"score\": float(score.item())\n",
    "                    }\n",
    "                    results.append(result)\n",
    "    \n",
    "    \n",
    "    with open('predictions.json', 'w') as f:\n",
    "        json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41d867d4-a97e-483c-8d9b-753ab6830dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_eval(path):\n",
    "\n",
    "    from pycocotools.coco import COCO\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "    \n",
    "    cocoGt = COCO(path + '/annotations/labels.json')  # Load the ground truth\n",
    "    cocoDt = cocoGt.loadRes('predictions.json')  # Load the predictions\n",
    "    \n",
    "    cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')  # Initialize COCOeval for bounding box evaluation\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0358947-04df-4bbc-b152-fb6a9942bf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/automl/lib/python3.11/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/usr/local/automl/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #0 loss: 3.95310378074646\n",
      "Iteration #0 loss: 1.1223046779632568\n",
      "Iteration #0 loss: 0.8583314418792725\n",
      "Iteration #0 loss: 0.5336757898330688\n",
      "Iteration #0 loss: 0.4925512671470642\n",
      "Iteration #0 loss: 0.43428030610084534\n",
      "Iteration #0 loss: 0.7340050339698792\n",
      "Iteration #0 loss: 0.48963961005210876\n",
      "Iteration #0 loss: 0.3516971468925476\n",
      "Iteration #0 loss: 0.6191205382347107\n",
      "Iteration #0 loss: 0.38004398345947266\n",
      "Iteration #0 loss: 0.38578230142593384\n",
      "Iteration #0 loss: 0.3294800817966461\n",
      "Iteration #0 loss: 0.3977416753768921\n",
      "Iteration #0 loss: 0.3722078204154968\n",
      "Iteration #0 loss: 0.37976548075675964\n",
      "Iteration #0 loss: 0.3547268509864807\n",
      "Iteration #0 loss: 0.3614848554134369\n",
      "Iteration #0 loss: 0.4079762399196625\n",
      "Iteration #0 loss: 0.46287861466407776\n",
      "Iteration #0 loss: 0.34341728687286377\n",
      "Iteration #0 loss: 0.3050822913646698\n",
      "Iteration #0 loss: 0.20184817910194397\n",
      "Iteration #0 loss: 0.20880062878131866\n",
      "Iteration #0 loss: 0.254375159740448\n",
      "Iteration #0 loss: 0.35746312141418457\n",
      "Iteration #0 loss: 0.23010076582431793\n",
      "Iteration #0 loss: 0.23852767050266266\n",
      "Iteration #0 loss: 0.33299121260643005\n",
      "Iteration #0 loss: 0.20882336795330048\n",
      "Iteration #0 loss: 0.24128112196922302\n",
      "Iteration #0 loss: 0.320875346660614\n",
      "Iteration #0 loss: 0.27724677324295044\n",
      "Iteration #0 loss: 0.3119943141937256\n",
      "Iteration #0 loss: 0.22623154520988464\n",
      "Iteration #0 loss: 0.2663222849369049\n",
      "Iteration #0 loss: 0.30400463938713074\n",
      "Iteration #0 loss: 0.2200239896774292\n",
      "Iteration #0 loss: 0.21376515924930573\n",
      "Iteration #0 loss: 0.26956120133399963\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.81s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.400\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.603\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.452\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.121\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.229\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.376\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.163\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.370\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.471\n",
      "CPU times: user 26min 17s, sys: 5.64 s, total: 26min 23s\n",
      "Wall time: 6min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_path = './tmp/data/coco_train_balanced/unzip/obj_dect_train_balanced/'\n",
    "test_path = './tmp/data/coco_test_balanced/unzip/obj_dect_test_balanced/'\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "train, val, test, test_ds = create_data(train_path, test_path)\n",
    "model = model_train(device, train, 40)\n",
    "model_eval(device, model, test, test_ds)\n",
    "coco_eval(test_path)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d87c1765-534d-4456-8c97-34b7dedaac81",
   "metadata": {},
   "source": [
    "Iteration #0 loss: 3.95310378074646\n",
    "Iteration #0 loss: 1.1223046779632568\n",
    "Iteration #0 loss: 0.8583314418792725\n",
    "Iteration #0 loss: 0.5336757898330688\n",
    "Iteration #0 loss: 0.4925512671470642\n",
    "Iteration #0 loss: 0.43428030610084534\n",
    "Iteration #0 loss: 0.7340050339698792\n",
    "Iteration #0 loss: 0.48963961005210876\n",
    "Iteration #0 loss: 0.3516971468925476\n",
    "Iteration #0 loss: 0.6191205382347107\n",
    "Iteration #0 loss: 0.38004398345947266\n",
    "Iteration #0 loss: 0.38578230142593384\n",
    "Iteration #0 loss: 0.3294800817966461\n",
    "Iteration #0 loss: 0.3977416753768921\n",
    "Iteration #0 loss: 0.3722078204154968\n",
    "Iteration #0 loss: 0.37976548075675964\n",
    "Iteration #0 loss: 0.3547268509864807\n",
    "Iteration #0 loss: 0.3614848554134369\n",
    "Iteration #0 loss: 0.4079762399196625\n",
    "Iteration #0 loss: 0.46287861466407776\n",
    "Iteration #0 loss: 0.34341728687286377\n",
    "Iteration #0 loss: 0.3050822913646698\n",
    "Iteration #0 loss: 0.20184817910194397\n",
    "Iteration #0 loss: 0.20880062878131866\n",
    "Iteration #0 loss: 0.254375159740448\n",
    "Iteration #0 loss: 0.35746312141418457\n",
    "Iteration #0 loss: 0.23010076582431793\n",
    "Iteration #0 loss: 0.23852767050266266\n",
    "Iteration #0 loss: 0.33299121260643005\n",
    "Iteration #0 loss: 0.20882336795330048\n",
    "Iteration #0 loss: 0.24128112196922302\n",
    "Iteration #0 loss: 0.320875346660614\n",
    "Iteration #0 loss: 0.27724677324295044\n",
    "Iteration #0 loss: 0.3119943141937256\n",
    "Iteration #0 loss: 0.22623154520988464\n",
    "Iteration #0 loss: 0.2663222849369049\n",
    "Iteration #0 loss: 0.30400463938713074\n",
    "Iteration #0 loss: 0.2200239896774292\n",
    "Iteration #0 loss: 0.21376515924930573\n",
    "Iteration #0 loss: 0.26956120133399963\n",
    "loading annotations into memory...\n",
    "Done (t=0.00s)\n",
    "creating index...\n",
    "index created!\n",
    "Loading and preparing results...\n",
    "DONE (t=0.00s)\n",
    "creating index...\n",
    "index created!\n",
    "Running per image evaluation...\n",
    "Evaluate annotation type *bbox*\n",
    "DONE (t=0.81s).\n",
    "Accumulating evaluation results...\n",
    "DONE (t=0.02s).\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.400\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.603\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.452\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.121\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.229\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.376\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.468\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.163\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.370\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.471\n",
    "CPU times: user 26min 17s, sys: 5.64 s, total: 26min 23s\n",
    "Wall time: 6min 44"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python automl",
   "language": "python",
   "name": "automl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
