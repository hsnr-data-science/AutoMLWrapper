{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "533c1d68-10c2-42c6-8594-29d5d993ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORFLOW = 1\n",
    "GLUON = 1\n",
    "\n",
    "import os  \n",
    "if GLUON:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "if TENSORFLOW:\n",
    "    import tensorflow as tf\n",
    "\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "            tf.config.set_logical_device_configuration(\n",
    "                gpus[0],\n",
    "                [tf.config.LogicalDeviceConfiguration(memory_limit=24000)])\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1063b58-ae43-4b82-8b13-dd1d66da9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, pandas as pd, requests, mlflow, sys, os, logging, numpy as np\n",
    "from automlwrapper import AutoMLWrapper, SedarDataLoader\n",
    "from sedarapi import SedarAPI\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.getLogger('automlwrapper').setLevel(logging.ERROR)\n",
    "\n",
    "np.random.seed(1313)\n",
    "\n",
    "TIME_LIMIT = 60 * 60\n",
    "AUTOKERAS_TRIALS = 20\n",
    "AUTOKERAS_EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85666f08-70b7-4a27-9d5f-3adbd1d3da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MLFLOW = False\n",
    "\n",
    "MLFLOW_URI = 'http://192.168.220.107:6798'\n",
    "MLFLOW_EXPID = '1'\n",
    "if USE_MLFLOW:\n",
    "    mlflow.set_tracking_uri(MLFLOW_URI) \n",
    "    mlflow.set_experiment(experiment_id=MLFLOW_EXPID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "336e9d03-3566-4333-be2b-48ef75557614",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEDAR_URI = 'http://192.168.220.107:5000'\n",
    "\n",
    "SEDAR = SedarAPI(SEDAR_URI)\n",
    "DataLoader = SedarDataLoader(SEDAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "210d83d9-d1d2-4b69-abb7-bd29c4fe0990",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = './tmp/data/pda_2023-04-18_10-13-22.csv'\n",
    "label_location = './tmp/data/labels_030723.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb74a94-b19d-417d-94d8-ceab6642669e",
   "metadata": {},
   "source": [
    "## download data to the above locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7d98ff2-7fa6-4019-9af7-67febbc98adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 14098  100 14098    0     0  41526      0 --:--:-- --:--:-- --:--:-- 41464\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 67.7M  100 67.7M    0     0  64.6M      0  0:00:01  0:00:01 --:--:-- 64.7M\n"
     ]
    }
   ],
   "source": [
    "!curl --create-dirs -O --output-dir \\\n",
    "./tmp/data \\\n",
    "https://gitlab.com/mibbels/automlwrapperdata/-/raw/main/tabular-regression/labels_030723.csv \n",
    "\n",
    "!curl --create-dirs -O --output-dir \\\n",
    "./tmp/data \\\n",
    "https://gitlab.com/mibbels/automlwrapperdata/-/raw/main/tabular-regression/pda_2023-04-18_10-13-22.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70314954-7d48-4ea8-8fe0-6f01325d80fb",
   "metadata": {},
   "source": [
    "# data preparation taken without changes \n",
    "Hoseini, S., et. al.: Coatings intelligence: Data-driven automation for chemistry\n",
    "4.0. In: 2024 IEEE 7th (ICPS). pp. 1â€“8 (2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bc7d911-8f07-44ec-b79d-8cfa8dc53518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398\n",
      "398\n",
      "398\n",
      "398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_619644/1150566557.py:84: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  row_value = int(indidvidual['row'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{519}\n"
     ]
    }
   ],
   "source": [
    "csv = pd.read_csv(data_location, skiprows = 0)\n",
    "csv['Zeit'] =  pd.to_datetime(csv['Zeit'])\n",
    "csv.sort_values(by='Zeit', inplace = True)\n",
    "\n",
    "labels = pd.read_csv(label_location)\n",
    "labels = labels[labels['row'] != 'None']\n",
    "labels = labels[labels['row'] != 'Aussortieren']\n",
    "print(len(labels))\n",
    "\n",
    "df = csv[['Zeit','product_id', 'run_id', 'experiment_id', 'trial_id', 'set_force_begin',\n",
    "       'x_position', 'y_position', 'z1_position', 'z2_position', 'x_velocity',\n",
    "       'y_velocity', 'z1_velocity', 'z2_velocity', 'x_force', 'y_force',\n",
    "       'z_force']]\n",
    "df = df[df[\"product_id\"] == 304]\n",
    "good_experiment_ids = [{\"run_id\": 0, \"experiment_ids\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
    "                  {\"run_id\": 1, \"experiment_ids\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
    "                  {\"run_id\": 2, \"experiment_ids\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
    "                  {\"run_id\": 3, \"experiment_ids\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\n",
    "                ]\n",
    "peak_dfs = []\n",
    "i = 0\n",
    "for item in good_experiment_ids:\n",
    "    df_temp = df[df[\"run_id\"] == item[\"run_id\"]]\n",
    "    for item2 in item[\"experiment_ids\"]:\n",
    "        df_temp2 = df_temp[df_temp[\"experiment_id\"] == item2]\n",
    "        liste_temp = df_temp2[\"trial_id\"].unique()\n",
    "        for item3 in liste_temp:\n",
    "            i += 1\n",
    "            #print(item[\"run_id\"], item2, item3 )\n",
    "            if labels[(labels['trial_id'] == item3) & (labels['experiment_id'] == item2) & (labels['run_id'] == item[\"run_id\"])].shape[0] > 0:\n",
    "                peak_df = df_temp2[df_temp2[\"trial_id\"] == item3]\n",
    "                peak_dfs.append(peak_df[[\"run_id\", \"trial_id\", \"experiment_id\", 'x_position', 'x_force', 'y_force', 'z_force']])\n",
    "                \n",
    "print(len(peak_dfs))\n",
    "\n",
    "filtered_peak_dfs = []\n",
    "\n",
    "for i, item in enumerate(peak_dfs):\n",
    "    filtered_df_temp = item[item['x_position'] > 20.0001].reset_index(drop=True)\n",
    "    \n",
    "    peak_row_temp = filtered_df_temp['x_position'].idxmax()\n",
    "    \n",
    "    peak_row_data_temp = filtered_df_temp.loc[:peak_row_temp-1]\n",
    "    \n",
    "    filtered_df_temp2 = filtered_df_temp.loc[peak_row_temp:]\n",
    "    \n",
    "    condition = filtered_df_temp2['x_force'] >= 0\n",
    "    \n",
    "    extracted_rows = filtered_df_temp2.loc[:condition.idxmax()]\n",
    "        \n",
    "    if (extracted_rows['x_position'] >= 99.9).all():\n",
    "        filtered_peak_dfs.append(pd.concat([peak_row_data_temp, extracted_rows]))\n",
    "    else:\n",
    "        filtered_peak_dfs.append(peak_row_data_temp)\n",
    "print(len(filtered_peak_dfs))\n",
    "\n",
    "max_length = max(len(df) for df in filtered_peak_dfs)\n",
    "\n",
    "padded_dataframes = []\n",
    "for df in filtered_peak_dfs:\n",
    "    padding_size = 519 - len(df) #padding_size - len(df) # check classification\n",
    "    padded_df = pd.DataFrame(np.pad(df.values, ((0, padding_size), (0, 0)), mode='edge'), columns=df.columns)\n",
    "    padded_df['index'] = padded_df.index\n",
    "    padded_dataframes.append(padded_df)\n",
    "print(len(padded_dataframes))\n",
    "\n",
    "lengths = set()\n",
    "polke_padded_dataframes_with_labels = []\n",
    "for item in padded_dataframes:\n",
    "    lengths.add(len(item))\n",
    "    \n",
    "    run_id = item[\"run_id\"].unique()[0],\n",
    "    trial_id = item[\"trial_id\"].unique()[0],\n",
    "    experiment_id = item[\"experiment_id\"].unique()[0]\n",
    "    \n",
    "    #print(\"RUN_ID:\", run_id,\"experiment_id:\",  experiment_id,\"trial_id:\", trial_id)\n",
    "\n",
    "    indidvidual = labels[labels[\"run_id\"] == run_id]\n",
    "    indidvidual = indidvidual[indidvidual[\"experiment_id\"] == experiment_id]\n",
    "    indidvidual = indidvidual[indidvidual[\"trial_id\"] == trial_id]\n",
    "    \n",
    "    try:\n",
    "        if indidvidual['row'].iloc[0].isnumeric():\n",
    "            row_value = int(indidvidual['row'])\n",
    "            polke_padded_dataframes_with_labels.append((item, row_value))\n",
    "        else:\n",
    "            continue                                       ### <<<----- added try block\n",
    "    except AttributeError as a:\n",
    "        continue\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0523f093-2fd9-4cb1-b079-6797927fcba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "255\n",
      "(204, 519, 1) (41, 519, 1) (204,) (41,) (10, 519, 1) (10,)\n"
     ]
    }
   ],
   "source": [
    "padded_dataframes_with_labels_combined = polke_padded_dataframes_with_labels\n",
    "\n",
    "\n",
    "tensor_X = []\n",
    "tensor_y = []\n",
    "for item in padded_dataframes_with_labels_combined:\n",
    "    #df_temp = item[0][['x_force', 'y_force', 'z_force']].copy()\n",
    "    #df_temp = item[0][['x_force', 'z_force']].copy()\n",
    "    df_temp = item[0][['x_force']].copy()\n",
    "    a = df_temp.to_numpy().astype(np.float32)\n",
    "    tensor_X.append(a)\n",
    "    tensor_y.append(item[1])\n",
    "print(len(tensor_X))\n",
    "print(len(tensor_y))\n",
    "\n",
    "# train test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(np.array(tensor_X), np.array(tensor_y), test_size=0.2, shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.8, shuffle=True)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_val.shape , y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed63e5a8-56fb-4459-9ef4-335140fd7b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(X_train.reshape(X_train.shape[:2]))\n",
    "train['label'] = y_train\n",
    "\n",
    "test = pd.DataFrame(X_test.reshape(X_test.shape[:2]))\n",
    "test['label'] = y_test \n",
    "\n",
    "val = pd.DataFrame(X_val.reshape(X_val.shape[:2]))\n",
    "val['label'] = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "137665cf-08be-45a3-8822-0dc15b98ead8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204, 520)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6aaca4b6-c769-4660-b6a7-fc58836892d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 520)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa0e6604-3c62-472b-b58a-9e70983a8fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 520)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f03c281-1013-4220-9620-0a5725c5b88a",
   "metadata": {},
   "source": [
    "# Optimizing a range of different predictors using AutoGluon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe075b6-4a45-461c-a931-6713955d255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def wrapper_medium(train_data,val_data, eval_metric):\n",
    "    \n",
    "    wrapper = AutoMLWrapper('autogluon')\n",
    "    wrapper.Train(\n",
    "        train_data=train_data,\n",
    "        validation_data=val_data,\n",
    "        target_column='label',\n",
    "        task_type='regression',\n",
    "        data_type='tabular',\n",
    "        problem_type='regression',\n",
    "        hyperparameters={'time_limit': TIME_LIMIT,\n",
    "                         'preset' : 'medium_quality',\n",
    "                        'eval_metric':eval_metric},\n",
    "    )\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce6a44-64f1-4af3-80c8-e358feb7e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w = wrapper_medium(train, val, 'mean_squared_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d186fa56-1426-485a-a25a-0eaec688c0f6",
   "metadata": {},
   "source": [
    "Presets specified: ['medium_quality']\n",
    "Beginning AutoGluon training ... Time limit = 3600s\n",
    "AutoGluon will save models to \"/home/jovyan/AutoMLOutput/autogluon1712656406.408233\"\n",
    "=================== System Info ===================\n",
    "AutoGluon Version:  1.0.0\n",
    "Python Version:     3.11.8\n",
    "Operating System:   Linux\n",
    "Platform Machine:   x86_64\n",
    "Platform Version:   #193-Ubuntu SMP Thu Mar 7 14:29:28 UTC 2024\n",
    "CPU Count:          104\n",
    "Memory Avail:       641.55 GB / 754.52 GB (85.0%)\n",
    "Disk Space Avail:   3133.98 GB / 3665.44 GB (85.5%)\n",
    "===================================================\n",
    "Train Data Rows:    204\n",
    "Train Data Columns: 519\n",
    "Tuning Data Rows:    10\n",
    "Tuning Data Columns: 519\n",
    "Label Column:       label\n",
    "Problem Type:       regression\n",
    "Preprocessing data ...\n",
    "Using Feature Generators to preprocess the data ...\n",
    "Fitting AutoMLPipelineFeatureGenerator...\n",
    "\tAvailable Memory:                    656951.73 MB\n",
    "\tTrain Data (Original)  Memory Usage: 0.42 MB (0.0% of available memory)\n",
    "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
    "\tStage 1 Generators:\n",
    "\t\tFitting AsTypeFeatureGenerator...\n",
    "\tStage 2 Generators:\n",
    "\t\tFitting FillNaFeatureGenerator...\n",
    "\tStage 3 Generators:\n",
    "\t\tFitting IdentityFeatureGenerator...\n",
    "\tStage 4 Generators:\n",
    "\t\tFitting DropUniqueFeatureGenerator...\n",
    "\tStage 5 Generators:\n",
    "\t\tFitting DropDuplicatesFeatureGenerator...\n",
    "\tUnused Original Features (Count: 6): ['513', '514', '515', '516', '517', '518']\n",
    "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
    "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
    "\t\tThese features do not need to be present at inference time.\n",
    "\t\t('float', []) : 6 | ['513', '514', '515', '516', '517', ...]\n",
    "\tTypes of features in original data (raw dtype, special dtypes):\n",
    "\t\t('float', []) : 513 | ['0', '1', '2', '3', '4', ...]\n",
    "\tTypes of features in processed data (raw dtype, special dtypes):\n",
    "\t\t('float', []) : 513 | ['0', '1', '2', '3', '4', ...]\n",
    "\t0.3s = Fit runtime\n",
    "\t513 features in original data used to generate 513 features in processed data.\n",
    "\tTrain Data (Processed) Memory Usage: 0.42 MB (0.0% of available memory)\n",
    "Data preprocessing and feature engineering runtime = 0.38s ...\n",
    "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
    "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
    "\tTo change this, specify the eval_metric parameter of Predictor()\n",
    "Warning: use_bag_holdout=True, but bagged mode is not enabled. use_bag_holdout will be ignored.\n",
    "User-specified model hyperparameters to be fit:\n",
    "{\n",
    "\t'NN_TORCH': {},\n",
    "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
    "\t'CAT': {},\n",
    "\t'XGB': {},\n",
    "\t'FASTAI': {},\n",
    "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
    "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
    "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
    "}\n",
    "Fitting 11 L1 models ...\n",
    "Fitting model: KNeighborsUnif ... Training model for up to 3599.62s of the 3599.61s of remaining time.\n",
    "\t-524.4919\t = Validation score   (-mean_squared_error)\n",
    "\t0.05s\t = Training   runtime\n",
    "\t0.17s\t = Validation runtime\n",
    "Fitting model: KNeighborsDist ... Training model for up to 3599.38s of the 3599.38s of remaining time.\n",
    "\t-454.2741\t = Validation score   (-mean_squared_error)\n",
    "\t0.07s\t = Training   runtime\n",
    "\t0.07s\t = Validation runtime\n",
    "Fitting model: LightGBMXT ... Training model for up to 3599.24s of the 3599.24s of remaining time.\n",
    "/usr/local/automl/lib/python3.11/site-packages/dask/dataframe/__init__.py:31: FutureWarning: \n",
    "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
    "\n",
    "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
    "This will raise in a future version.\n",
    "\n",
    "  warnings.warn(msg, FutureWarning)\n",
    "\t-146.0729\t = Validation score   (-mean_squared_error)\n",
    "\t1.78s\t = Training   runtime\n",
    "\t0.0s\t = Validation runtime\n",
    "Fitting model: LightGBM ... Training model for up to 3597.44s of the 3597.44s of remaining time.\n",
    "\t-814.0357\t = Validation score   (-mean_squared_error)\n",
    "\t1.53s\t = Training   runtime\n",
    "\t0.0s\t = Validation runtime\n",
    "Fitting model: RandomForestMSE ... Training model for up to 3595.9s of the 3595.9s of remaining time.\n",
    "\t-922.272\t = Validation score   (-mean_squared_error)\n",
    "\t1.17s\t = Training   runtime\n",
    "\t0.11s\t = Validation runtime\n",
    "Fitting model: CatBoost ... Training model for up to 3594.57s of the 3594.56s of remaining time.\n",
    "\t-574.5555\t = Validation score   (-mean_squared_error)\n",
    "\t7.97s\t = Training   runtime\n",
    "\t0.02s\t = Validation runtime\n",
    "Fitting model: ExtraTreesMSE ... Training model for up to 3586.56s of the 3586.55s of remaining time.\n",
    "\t-717.0577\t = Validation score   (-mean_squared_error)\n",
    "\t1.14s\t = Training   runtime\n",
    "\t0.12s\t = Validation runtime\n",
    "Fitting model: NeuralNetFastAI ... Training model for up to 3585.24s of the 3585.24s of remaining time.\n",
    "\t-291.6507\t = Validation score   (-mean_squared_error)\n",
    "\t1.66s\t = Training   runtime\n",
    "\t0.01s\t = Validation runtime\n",
    "Fitting model: XGBoost ... Training model for up to 3583.54s of the 3583.54s of remaining time.\n",
    "\t-1023.6663\t = Validation score   (-mean_squared_error)\n",
    "\t2.08s\t = Training   runtime\n",
    "\t0.01s\t = Validation runtime\n",
    "Fitting model: NeuralNetTorch ... Training model for up to 3581.44s of the 3581.44s of remaining time.\n",
    "\t-144.9334\t = Validation score   (-mean_squared_error)\n",
    "\t2.68s\t = Training   runtime\n",
    "\t0.14s\t = Validation runtime\n",
    "Fitting model: LightGBMLarge ... Training model for up to 3578.61s of the 3578.61s of remaining time.\n",
    "\t-1911.1091\t = Validation score   (-mean_squared_error)\n",
    "\t2.9s\t = Training   runtime\n",
    "\t0.0s\t = Validation runtime\n",
    "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3575.68s of remaining time.\n",
    "\tEnsemble Weights: {'LightGBMXT': 0.482, 'NeuralNetTorch': 0.482, 'KNeighborsDist': 0.035}\n",
    "\t-69.188\t = Validation score   (-mean_squared_error)\n",
    "\t0.33s\t = Training   runtime\n",
    "\t0.0s\t = Validation runtime\n",
    "AutoGluon training complete, total runtime = 24.68s ... Best model: \"WeightedEnsemble_L2\"\n",
    "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/jovyan/AutoMLOutput/autogluon1712656406.408233\")\n",
    "\n",
    "CPU times: user 12min 41s, sys: 22.9 s, total: 13min 4s\n",
    "Wall time: 24.7 s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbecdb6-b119-436b-8f99-822b26be700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "extra_kwargs_for_auto_gluon = {'auxiliary_metrics' : True, 'detailed_report' : True}\n",
    "\n",
    "re = w.Evaluate(test, target_column='Type', **extra_kwargs_for_auto_gluon)\n",
    "print(re)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d90a2d09-e21a-4dd7-b8bf-eb369ae87446",
   "metadata": {},
   "source": [
    "{'mean_squared_error': -190.55400356617983, 'root_mean_squared_error': -13.80412994600456, 'mean_absolute_error': -8.858933983779535, 'r2': 0.982667071994958, 'pearsonr': 0.9925373887125394, 'median_absolute_error': -5.50408935546875}\n",
    "CPU times: user 2.39 s, sys: 61.5 ms, total: 2.45 s\n",
    "Wall time: 234 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc228e5-2fcb-4a19-afd8-cee6d8eba3f9",
   "metadata": {},
   "source": [
    "# Optimizing neural networks for tabular regression using AutoKeras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fb5caf-19dc-4e08-af13-3193156310d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_keras(train_data, test_data):\n",
    "    import keras_tuner \n",
    "    \n",
    "    wrapper = AutoMLWrapper('autokeras')\n",
    "    wrapper.Train(\n",
    "        train_data=train_data,\n",
    "        target_column='label',\n",
    "        task_type='regression',\n",
    "        data_type='tabular',\n",
    "        problem_type='regression',\n",
    "        hyperparameters={'evaluation_metric':['mean_squared_error', 'mean_absolute_error'],\n",
    "                         'epochs':AUTOKERAS_EPOCHS,\n",
    "                         'num_trials':AUTOKERAS_TRIALS,\n",
    "                        \n",
    "                         'objective_metric':keras_tuner.Objective(\"val_mean_squared_error\", direction=\"min\")}\n",
    "    )\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8b3a5-88c7-47b9-a95c-6953bbfea6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "w = wrapper_keras(train, test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc226284-2fbd-48bd-b84b-44c03d83548e",
   "metadata": {},
   "source": [
    "Trial 20 Complete [00h 00m 20s]\n",
    "val_mean_squared_error: 5367.74658203125\n",
    "\n",
    "Best val_mean_squared_error So Far: 2203.679443359375\n",
    "Total elapsed time: 00h 06m 35s\n",
    "Epoch 1/30\n",
    "7/7 [==============================] - 4s 26ms/step - loss: 72295.1641 - mean_squared_error: 72295.1641 - mean_absolute_error: 227.1230\n",
    "Epoch 2/30\n",
    "7/7 [==============================] - 0s 22ms/step - loss: 22248.2520 - mean_squared_error: 22248.2520 - mean_absolute_error: 130.0026\n",
    "Epoch 3/30\n",
    "7/7 [==============================] - 0s 26ms/step - loss: 19569.2402 - mean_squared_error: 19569.2402 - mean_absolute_error: 112.6005\n",
    "Epoch 4/30\n",
    "7/7 [==============================] - 0s 25ms/step - loss: 11457.1240 - mean_squared_error: 11457.1240 - mean_absolute_error: 89.7696\n",
    "Epoch 5/30\n",
    "7/7 [==============================] - 0s 24ms/step - loss: 7052.3750 - mean_squared_error: 7052.3750 - mean_absolute_error: 66.3073\n",
    "Epoch 6/30\n",
    "7/7 [==============================] - 0s 25ms/step - loss: 6404.6685 - mean_squared_error: 6404.6685 - mean_absolute_error: 59.8612\n",
    "Epoch 7/30\n",
    "7/7 [==============================] - 0s 24ms/step - loss: 4540.1846 - mean_squared_error: 4540.1846 - mean_absolute_error: 51.3146\n",
    "Epoch 8/30\n",
    "7/7 [==============================] - 0s 24ms/step - loss: 4197.6206 - mean_squared_error: 4197.6206 - mean_absolute_error: 50.7489\n",
    "Epoch 9/30\n",
    "7/7 [==============================] - 0s 29ms/step - loss: 3783.6042 - mean_squared_error: 3783.6042 - mean_absolute_error: 45.9164\n",
    "Epoch 10/30\n",
    "7/7 [==============================] - 0s 26ms/step - loss: 2834.6602 - mean_squared_error: 2834.6602 - mean_absolute_error: 39.1356\n",
    "Epoch 11/30\n",
    "7/7 [==============================] - 0s 25ms/step - loss: 2586.9751 - mean_squared_error: 2586.9751 - mean_absolute_error: 38.5874\n",
    "Epoch 12/30\n",
    "7/7 [==============================] - 0s 26ms/step - loss: 2374.7322 - mean_squared_error: 2374.7322 - mean_absolute_error: 37.0404\n",
    "Epoch 13/30\n",
    "7/7 [==============================] - 0s 27ms/step - loss: 1998.0836 - mean_squared_error: 1998.0836 - mean_absolute_error: 33.8315\n",
    "Epoch 14/30\n",
    "7/7 [==============================] - 0s 25ms/step - loss: 1590.4092 - mean_squared_error: 1590.4092 - mean_absolute_error: 30.0497\n",
    "Epoch 15/30\n",
    "7/7 [==============================] - 0s 29ms/step - loss: 1225.9238 - mean_squared_error: 1225.9238 - mean_absolute_error: 26.3521\n",
    "Epoch 16/30\n",
    "7/7 [==============================] - 0s 29ms/step - loss: 990.9075 - mean_squared_error: 990.9075 - mean_absolute_error: 23.9214\n",
    "Epoch 17/30\n",
    "7/7 [==============================] - 0s 27ms/step - loss: 852.7446 - mean_squared_error: 852.7446 - mean_absolute_error: 21.9643\n",
    "Epoch 18/30\n",
    "7/7 [==============================] - 0s 24ms/step - loss: 759.1860 - mean_squared_error: 759.1860 - mean_absolute_error: 20.9191\n",
    "Epoch 19/30\n",
    "7/7 [==============================] - 0s 25ms/step - loss: 647.0060 - mean_squared_error: 647.0060 - mean_absolute_error: 19.3680\n",
    "Epoch 20/30\n",
    "7/7 [==============================] - 0s 21ms/step - loss: 665.4276 - mean_squared_error: 665.4276 - mean_absolute_error: 19.4590\n",
    "Epoch 21/30\n",
    "7/7 [==============================] - 0s 25ms/step - loss: 784.2915 - mean_squared_error: 784.2915 - mean_absolute_error: 21.2611\n",
    "Epoch 22/30\n",
    "7/7 [==============================] - 0s 24ms/step - loss: 1032.1973 - mean_squared_error: 1032.1973 - mean_absolute_error: 24.3092\n",
    "Epoch 23/30\n",
    "7/7 [==============================] - 0s 25ms/step - loss: 1611.9098 - mean_squared_error: 1611.9098 - mean_absolute_error: 30.2321\n",
    "Epoch 24/30\n",
    "7/7 [==============================] - 0s 25ms/step - loss: 2790.5369 - mean_squared_error: 2790.5369 - mean_absolute_error: 38.7756\n",
    "Epoch 25/30\n",
    "7/7 [==============================] - 0s 24ms/step - loss: 4460.7798 - mean_squared_error: 4460.7798 - mean_absolute_error: 49.2092\n",
    "Epoch 26/30\n",
    "7/7 [==============================] - 0s 25ms/step - loss: 4755.3433 - mean_squared_error: 4755.3433 - mean_absolute_error: 51.3135\n",
    "Epoch 27/30\n",
    "7/7 [==============================] - 0s 25ms/step - loss: 3904.6914 - mean_squared_error: 3904.6914 - mean_absolute_error: 50.3322\n",
    "Epoch 28/30\n",
    "7/7 [==============================] - 0s 25ms/step - loss: 5070.9688 - mean_squared_error: 5070.9688 - mean_absolute_error: 56.0891\n",
    "Epoch 29/30\n",
    "7/7 [==============================] - 0s 27ms/step - loss: 4771.6548 - mean_squared_error: 4771.6548 - mean_absolute_error: 56.1124\n",
    "Epoch 30/30\n",
    "7/7 [==============================] - 0s 24ms/step - loss: 2284.5337 - mean_squared_error: 2284.5337 - mean_absolute_error: 37.4259\n",
    "INFO:tensorflow:Assets written to: /home/jovyan/AutoMLOutput/autokeras1712656447.573243/structured_data_regressor/best_model/assets\n",
    "\n",
    "INFO:tensorflow:Assets written to: /home/jovyan/AutoMLOutput/autokeras1712656447.573243/structured_data_regressor/best_model/assets\n",
    "\n",
    "CPU times: user 8min 22s, sys: 1min 56s, total: 10min 18s\n",
    "Wall time: 6min 56s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd9dee-ea56-4084-bcb2-caebbad4652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "re = w.Evaluate(test, target_column='label', return_dict = True)\n",
    "print(re)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "571bd87e-d988-4664-9783-6949e2c02513",
   "metadata": {},
   "source": [
    "2/2 [==============================] - 2s 30ms/step - loss: 5902.6943 - mean_squared_error: 5902.6943 - mean_absolute_error: 64.6829\n",
    "{'loss': 5902.6943359375, 'mean_squared_error': 5902.6943359375, 'mean_absolute_error': 64.68287658691406}\n",
    "CPU times: user 3.52 s, sys: 265 ms, total: 3.79 s\n",
    "Wall time: 3.59 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed777c-7bc8-4ba8-bca2-ac43789c29a3",
   "metadata": {},
   "source": [
    "# Optimizing a range of different predictors using AutoSklearn\n",
    "\n",
    "### kernel needs to be changed from 'automl' to 'AutoSklearn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcc829c5-281f-4e7a-9ec7-6f896ce86ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_sk(train_data):\n",
    "    \n",
    "    wrapper = AutoMLWrapper('autosklearn')\n",
    "    wrapper.Train(\n",
    "        train_data=train_data,\n",
    "        target_column='label',\n",
    "        task_type='regression',\n",
    "        data_type='tabular',\n",
    "        problem_type='regression',\n",
    "        hyperparameters={'time_limit': TIME_LIMIT, \n",
    "                         'evaluation_metric':'mean_absolute_error'},\n",
    "    )\n",
    "\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8fae7-4b05-4179-866d-98eac2ed1dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2024-04-09 11:09:08,741:Client-AutoML(1):9466601e-f661-11ee-b47c-362a8405ebd2] Capping the per_run_time_limit to 1799.0 to have time for a least 2 models in each process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/autosklearn/lib/python3.8/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 33593 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w = wrapper_sk(train)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "edada147-3586-45a7-9d1a-ba353bc23b20",
   "metadata": {},
   "source": [
    "[WARNING] [2024-04-09 11:09:08,741:Client-AutoML(1):9466601e-f661-11ee-b47c-362a8405ebd2] Capping the per_run_time_limit to 1799.0 to have time for a least 2 models in each process.\n",
    "\n",
    "/opt/conda/envs/autosklearn/lib/python3.8/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
    "Perhaps you already have a cluster running?\n",
    "Hosting the HTTP server on port 33593 instead\n",
    "  warnings.warn(\n",
    "\n",
    "CPU times: user 29min 21s, sys: 2min 37s, total: 31min 59s\n",
    "Wall time: 1h 2s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42128921-9980-4dc3-ae26-e89dee1dbfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ev = w.Evaluate(test, target_column='label',detailed_report=True)\n",
    "print(ev)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f7ddaff-ddf9-484d-9907-4d98d0d5f287",
   "metadata": {},
   "source": [
    "7.384142148785475\n",
    "CPU times: user 38.4 s, sys: 6.87 s, total: 45.2 s\n",
    "Wall time: 1.39 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140499b-3ea5-4bdd-b747-926309efe272",
   "metadata": {},
   "source": [
    "-------------\n",
    "# Evaluating the LLMs capabilities when creating regression code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdff7974-c5e3-4fab-9c5a-090821109df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[:2])\n",
    "train['label'] = y_train\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[:2])\n",
    "test['label'] = y_test \n",
    "\n",
    "X_val = X_val.reshape(X_val.shape[:2])\n",
    "val['label'] = y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223ed72f-def3-474b-8002-67ae09dd940a",
   "metadata": {},
   "source": [
    "# **Trial 1**: This prompt is an adaption of the standard SEDAR version for using numpy arrays instead of dataframes\n",
    "Generate a Python code snippet for a tabular regression pipeline. The data is contained in numpy array X and y for train, validation, and test respectively. The data is of shape (# samples, 519, 1), i.e., it contains 519 features. Focus on a simple yet effective architecture. Include necessary imports, dataset loading and preprocessing steps, model creating, training and testing. Annotate the code with comments to explain each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b834de5-63d9-4ab8-8ad1-e35ebfd8a580",
   "metadata": {},
   "source": [
    "# OpenOrca's answer is a basic linear regression model with minimal feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "098bc041-33be-413d-a892-35771f648dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 8826.39593681153\n",
      "MAE: 63.87525493342702\n",
      "CPU times: user 349 ms, sys: 1.58 s, total: 1.93 s\n",
      "Wall time: 44 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "#Scaling:\n",
    "scaler = StandardScaler()  # Use a scaler to normalize the features (optional)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)               \n",
    "#Creating a basic model and fitting it to the data:\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [('Scaler', scaler), ('Regressor', LinearRegression())]\n",
    ") \n",
    "# Create a pipeline containing the scaler and linear regression model, add more steps if necessary\n",
    "model = pipeline.fit(X_train_scaled, y_train) # Fit the model to the training data\n",
    "#Evaluation:\n",
    "\n",
    "y_pred = model.predict(X_test_scaled) # Predict on validation set using the trained model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "# Calculate MSE for validationset\n",
    "print(\"MSE:\", mse)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", mae)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5dee9dbf-88d6-4a0a-944e-f4e583610fbc",
   "metadata": {},
   "source": [
    "MSE: 8826.39593681153\n",
    "MAE: 63.87525493342702\n",
    "CPU times: user 349 ms, sys: 1.58 s, total: 1.93 s\n",
    "Wall time: 44 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70307195-0e33-44a5-9dc4-4233743e2809",
   "metadata": {},
   "source": [
    "# **Trial 2**: Even though there were no mentions of the fact that the model can be greatly improved, we asked for improvements and implementations\n",
    "\n",
    "Prompt 2:\n",
    "\n",
    "The performance of this model is not good enough. Give me enhancement proposals and show how to implement them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b58d57-3274-4c2a-8508-368d03678a3c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Answer 2:\n",
    "\n",
    "Here are some enhancement proposals and their implementations to improve the model's performance:\n",
    "\n",
    "    Feature Selection: Remove irrelevant or redundant features from your dataset. This can help in reducing overfitting and improving generalization capabilities of the model.\n",
    "\n",
    "    Regularization: Adding regularization techniques like Lasso, Ridge Regression, or Elastic Net to your LinearRegression model could improve its performance by preventing overfitting and promoting sparse models.\n",
    "\n",
    "    Ensemble Methods: Combine multiple models (e.g., Bagging, Random Forest, Gradient Boosting) to create an ensemble model that can outperform individual models in terms of accuracy and stability.\n",
    "\n",
    "    Hyperparameter Tuning: Optimize the hyperparameters of your model using techniques like Grid Search or Random Search to find the best combination for your dataset. This could lead to a significant improvement in performance.\n",
    "\n",
    "    Model Selection: Experiment with different types of models, such as Decision Trees, Neural Networks, or Support Vector Machines, and compare their performances on your dataset to choose the most suitable one.\n",
    "\n",
    "Here's an example implementation for each proposal:\n",
    "\n",
    "    Feature Selection\n",
    "\n",
    "    ######################### answer ended here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479deeb-9ad6-451f-b764-e72d775c9be2",
   "metadata": {},
   "source": [
    "Prompt 3:\n",
    "\n",
    "Implement those changes in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb84c784-9900-4261-8dbc-471d0c330c86",
   "metadata": {},
   "source": [
    "Answer 3:\n",
    "\n",
    "Here is how you can implement feature selection:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selector = SelectKBest(score_func=chi2, k=5)  # Choose top 5 features based on Chi-squared statistics\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "```\n",
    "    Regularization: Lasso Regression\n",
    "######################### answer ended here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c6cfaf-81cb-43f9-87e7-6afb71f5fef1",
   "metadata": {},
   "source": [
    "## the  chi-squared is not appropriate  for the data due to negative values and had to be changed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59142f0f-40ad-4992-93bf-952645e18737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 4439.81570798467\n",
      "MAE: 53.39850932795827\n",
      "CPU times: user 55.5 ms, sys: 621 ms, total: 676 ms\n",
      "Wall time: 16.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression\n",
    "\n",
    "\n",
    "#Scaling:\n",
    "scaler = StandardScaler()  # Use a scaler to normalize the features (optional)\n",
    "\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)               \n",
    "#Creating a model and fitting it to the data:\n",
    "\n",
    "# Choose top 5 features based on Chi-squared statistics\n",
    "##selector = SelectKBest(score_func=chi2, k=5)  \n",
    "\n",
    "# EDITED, chi squared not working\n",
    "selector = SelectKBest(score_func=f_regression, k=10)  \n",
    "\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [('Scaler', scaler), ('Regressor', LinearRegression())]\n",
    ") \n",
    "# Create a pipeline containing the scaler and linear regression model\n",
    "model = pipeline.fit(X_train_selected, y_train) # Fit the model to the training data\n",
    "#Evaluation:\n",
    "\n",
    "y_pred = model.predict(X_test_selected) # Predict on validation set using the trained model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "# Calculate MSE for validationset\n",
    "print(\"MSE:\", mse)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa17ef03-f394-4c59-88a9-72d90fbd08a4",
   "metadata": {},
   "source": [
    "MSE: 4439.81570798467\n",
    "MAE: 53.39850932795827\n",
    "CPU times: user 55.5 ms, sys: 621 ms, total: 676 ms\n",
    "Wall time: 16.3 ms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python AutoSklearn",
   "language": "python",
   "name": "autosklearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
